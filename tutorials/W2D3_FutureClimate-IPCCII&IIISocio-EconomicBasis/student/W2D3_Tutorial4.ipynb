{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "rmt_vWoCB1Qc",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Tutorial 5 - Text analysis of COP tweets\n",
    "\n",
    "Week 2, Day 3: IPCC Socio-economic Basis\n",
    "\n",
    "By Neuromatch Academy\n",
    "\n",
    "Content creators: Name Surname, Name Surname, Maximilian Puelma Touzel\n",
    "\n",
    "Content reviewers: Name Surname, Name Surname\n",
    "\n",
    "Content editors: Name Surname, Name Surname\n",
    "\n",
    "Production editors: Name Surname, Name Surname\n",
    "\n",
    "Our 2023 Sponsors\n",
    "\n",
    "---\n",
    "# Tutorial Objective \n",
    "\n",
    "Learn to assess public sentiment about climate change and how it is changing by analyzing changes in word use on social media. An auxillary objective is to learn how to load and analyze large amounts of text\n",
    "\n",
    "---\n",
    "# Setup\n",
    "\n",
    "Import libraries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9553645a",
   "metadata": {
    "execution": {},
    "tags": [
     "colab"
    ]
   },
   "outputs": [],
   "source": [
    "# !pip install afinn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AM-4KSggB_xR",
   "metadata": {
    "execution": {},
    "executionInfo": {
     "elapsed": 27541,
     "status": "ok",
     "timestamp": 1682441197348,
     "user": {
      "displayName": "Maximilian Puelma Touzel",
      "userId": "09308600515315501700"
     },
     "user_tz": 240
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as pl\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "sns.set_style(\"ticks\", {'axes.grid' : False})\n",
    "#notebook config\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "from IPython.display import Math\n",
    "import glob\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "import urllib3\n",
    "import urllib.request  # the lib that handles the url stuff\n",
    "from afinn import Afinn\n",
    "import pooch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8wb9H2qOC1e-",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Mount local drive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8aa9343",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b726e00",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ecfd31",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We have done the following preprocessing steps for you (just read along; no need to run anything in this section):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b7ffe1",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Twitter ids of tweets mentioning COPx (x=20-26) used in Falkenberg et al. (2022) were placed by the authors in an osf archive: https://osf.io/nu75j. Download the 7 .csv files (one for each COP) here: https://osf.io/download/pr29x/ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c20cf26",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The program `twarc2` interfaces with the Twitter API. It can be used to hydrate the tweets (i.e. download the full tweet and metadata using the provided tweet id). Similar to github you have to open a Twitter API account and set up twarc on your local machine by giving it your account authentication keys. The command to rehyrdate a set of tweets from their ids is simply `twarc2 hydrate source_file.txt store_file.jsonl`, where each line of `source_file.txt` is a Twitter id and `store_file.jsonl` is where the hydrated tweets are stored."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f124e1",
   "metadata": {
    "execution": {}
   },
   "source": [
    "- First format the downloaded ids and split them into seperate files (*batches*) to make hydration calls to the API more time manageable (hours versus days - this is slow because of an API-imposed limit of 100 tweets/min.). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ceb30ff",
   "metadata": {
    "execution": {},
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1682441198212,
     "user": {
      "displayName": "Maximilian Puelma Touzel",
      "userId": "09308600515315501700"
     },
     "user_tz": 240
    }
   },
   "outputs": [],
   "source": [
    "# dir_name='Falkenberg2022_data/'\n",
    "# if not os.path.exists(dir_name):\n",
    "#     os.mkdir(dir_name)\n",
    "# batch_size = int(1e5)\n",
    "# download_pathname=''#~/projects/ClimateMatch/SocioEconDay/Polarization/COP_Twitter_IDs/\n",
    "# for copid in range(20,27):\n",
    "#     df_tweetids=pd.read_csv(download_pathname+'tweet_ids_cop'+str(copid)+'.csv')\n",
    "#     for batch_id,break_id in enumerate(range(0,len(df_tweetids),batch_size)):\n",
    "#         file_name=\"tweetids_COP\"+str(copid)+\"_b\"+str(batch_id)+\".txt\"\n",
    "#         df_tweetids.loc[break_id:break_id+batch_size,'id'].to_csv(dir_name+file_name,index=False,header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f668a3",
   "metadata": {
    "execution": {}
   },
   "source": [
    "- Make the hydration calls for a COP (this took 4 days to download 50GB of data for COP26)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4f4026",
   "metadata": {
    "execution": {},
    "executionInfo": {
     "elapsed": 314,
     "status": "ok",
     "timestamp": 1682441198517,
     "user": {
      "displayName": "Maximilian Puelma Touzel",
      "userId": "09308600515315501700"
     },
     "user_tz": 240
    }
   },
   "outputs": [],
   "source": [
    "# copid=26\n",
    "# filename_list = glob.glob('Falkenberg2022_data/'+\"tweetids_COP\"+str(copid)+\"*\")\n",
    "# dir_name='tweet_data/'\n",
    "# if not os.path.exists(dir_name):\n",
    "#     os.mkdir(dir_name)\n",
    "# file_name=\"tweetids_COP\"+str(copid)+\"_b\"+str(batch_id)+\".txt\"\n",
    "# for itt,tweet_id_batch_filename in enumerate(filename_list):\n",
    "#     strvars=tweet_id_batch_filename.split('/')[1].split('.')[0].split('_')\n",
    "#     tweet_store_filename = dir_name+'tweets_'+strvars[1]+'_'+strvars[2]+'.json'\n",
    "#     if not os.path.exists(tweet_store_filename):\n",
    "#         st=time.time()\n",
    "#         os.system('twarc2 hydrate '+tweet_id_batch_filename+' '+tweet_store_filename)\n",
    "#         print(str(itt)+' '+str(strvars[2])+\" \"+str(time.time()-st))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ef1f97",
   "metadata": {
    "execution": {}
   },
   "source": [
    "- Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ade1f98",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Inspect and pick a chunk size. Note, by default there are 100 tweets per line in the .json files returned by the API. Given we asked for 1e5 tweets/batch, there should be 1e3 lines in these files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc599db",
   "metadata": {
    "execution": {},
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1682441198518,
     "user": {
      "displayName": "Maximilian Puelma Touzel",
      "userId": "09308600515315501700"
     },
     "user_tz": 240
    }
   },
   "outputs": [],
   "source": [
    "# copid=26\n",
    "# batch_id = 0\n",
    "# tweet_store_filename = 'tweet_data/tweets_COP'+str(copid)+'_b'+str(batch_id)+'.json'\n",
    "# num_lines = sum(1 for line in open(tweet_store_filename))\n",
    "# num_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8985e74d",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now we read in the data, iterating over chunks in each batch and only store the needed data in a dataframe (takes 10-20 minutes to run). Let's look at when the tweets were posted, what language they are in, and the tweet text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c5364c",
   "metadata": {
    "execution": {},
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1682441198519,
     "user": {
      "displayName": "Maximilian Puelma Touzel",
      "userId": "09308600515315501700"
     },
     "user_tz": 240
    }
   },
   "outputs": [],
   "source": [
    "# selected_columns = ['created_at','lang','text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5194e171",
   "metadata": {
    "execution": {},
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1682441198520,
     "user": {
      "displayName": "Maximilian Puelma Touzel",
      "userId": "09308600515315501700"
     },
     "user_tz": 240
    }
   },
   "outputs": [],
   "source": [
    "# st=time.time()\n",
    "# filename_list = glob.glob('tweet_data/'+\"tweets_COP\"+str(copid)+\"*\")\n",
    "# df=[]\n",
    "# for tweet_batch_filename in filename_list[:-1]:\n",
    "#     reader = pd.read_json(tweet_batch_filename, lines=True,chunksize=1)\n",
    "# #     df.append(pd.DataFrame([item[selected_columns] for sublist in reader.data.values.tolist()[:-1] for item in sublist] )[selected_columns])\n",
    "#     dfs=[]\n",
    "#     for chunk in reader:\n",
    "#         if 'data' in chunk.columns:\n",
    "#             dfs.append(pd.DataFrame(list(chunk.data.values)[0])[selected_columns])\n",
    "#     df.append(pd.concat(dfs,ignore_index=True))\n",
    "# #     df.append(pd.DataFrame(list(reader.data)[0])[selected_columns])\n",
    "# df=pd.concat(df,ignore_index=True)\n",
    "# df.created_at=pd.to_datetime(df.created_at)\n",
    "# print(str(len(df))+' tweets took '+str(time.time()-st))\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c4a70a",
   "metadata": {
    "execution": {}
   },
   "source": [
    "- finally store the data in the efficiently compressed feather format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c092c48",
   "metadata": {
    "execution": {},
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1682441198520,
     "user": {
      "displayName": "Maximilian Puelma Touzel",
      "userId": "09308600515315501700"
     },
     "user_tz": 240
    }
   },
   "outputs": [],
   "source": [
    "# df.to_feather('stored_tweets')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c5e425",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Analysis (run notebook from here)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0xYALOlVOIaq",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Section 1: load and inspect data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60c41ba",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Load the preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa12333",
   "metadata": {
    "execution": {},
    "executionInfo": {
     "elapsed": 24315,
     "status": "ok",
     "timestamp": 1682441222825,
     "user": {
      "displayName": "Maximilian Puelma Touzel",
      "userId": "09308600515315501700"
     },
     "user_tz": 240
    }
   },
   "outputs": [],
   "source": [
    "fname = 'stored_tweets'\n",
    "if not os.path.exists(fname):\n",
    "    url = \"https://osf.io/8p52x/download\"\n",
    "    fname = pooch.retrieve(url, known_hash=None, fname=fname)\n",
    "df=pd.read_feather(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4739ce10",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Let's check the timing of the tweets relative to the COP26 event to see how the tweets are spread over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9f8eb0",
   "metadata": {
    "execution": {},
    "executionInfo": {
     "elapsed": 13946,
     "status": "ok",
     "timestamp": 1682441236740,
     "user": {
      "displayName": "Maximilian Puelma Touzel",
      "userId": "09308600515315501700"
     },
     "user_tz": 240
    }
   },
   "outputs": [],
   "source": [
    "total_tweetCounts=df.created_at.groupby(df.created_at.apply(lambda x: x.date)).count().rename('counts')\n",
    "total_tweetCounts.reset_index().plot(x='created_at',y='counts',figsize=(20,5),style='.-')\n",
    "pl.xticks(rotation=45,ha='right');\n",
    "pl.gca().set_yscale('log')\n",
    "COPdates=[datetime.datetime(2021,10,31), datetime.datetime(2021,11,12)] #shade the duration of the COP26 to guide the eye\n",
    "pl.gca().axvspan(*COPdates, alpha=0.3); #gray region"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4284f8e",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Who is tweeting about this COP? Look at how many tweets were posted in various languages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea608af6",
   "metadata": {
    "execution": {},
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1682441237444,
     "user": {
      "displayName": "Maximilian Puelma Touzel",
      "userId": "09308600515315501700"
     },
     "user_tz": 240
    }
   },
   "outputs": [],
   "source": [
    "counts=df.lang.value_counts().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3c8365",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The stored language name of the tweet is stored as a code name. Pull a language code dictionary from the web and use it to translate the language code to the language name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcf6f0d",
   "metadata": {
    "execution": {},
    "executionInfo": {
     "elapsed": 172,
     "status": "ok",
     "timestamp": 1682441237888,
     "user": {
      "displayName": "Maximilian Puelma Touzel",
      "userId": "09308600515315501700"
     },
     "user_tz": 240
    }
   },
   "outputs": [],
   "source": [
    "target_url='https://gist.githubusercontent.com/carlopires/1262033/raw/c52ef0f7ce4f58108619508308372edd8d0bd518/gistfile1.txt'\n",
    "exec(urllib.request.urlopen(target_url).read())\n",
    "lang_code_dict=dict(iso_639_choices)\n",
    "counts=counts.replace({'index':lang_code_dict})\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714e60db",
   "metadata": {
    "execution": {}
   },
   "source": [
    "*Exercise*: find your native language code in the dictionary and use it to select the COP tweets that were written in your language!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a69dc4",
   "metadata": {
    "execution": {},
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1682441237889,
     "user": {
      "displayName": "Maximilian Puelma Touzel",
      "userId": "09308600515315501700"
     },
     "user_tz": 240
    }
   },
   "outputs": [],
   "source": [
    "lang_code_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9723df",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Select tweets of a given language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf4eb4b",
   "metadata": {
    "execution": {},
    "executionInfo": {
     "elapsed": 6907,
     "status": "ok",
     "timestamp": 1682441245325,
     "user": {
      "displayName": "Maximilian Puelma Touzel",
      "userId": "09308600515315501700"
     },
     "user_tz": 240
    }
   },
   "outputs": [],
   "source": [
    "language_code = 'en'\n",
    "df=df.loc[df.lang==language_code,:].reset_index(drop=True)\n",
    "pd.options.display.max_rows=100 #see up to 100 entries\n",
    "pd.options.display.max_colwidth=250 #widen how much text is presented of each tweet\n",
    "df.sample(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc20327d",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 2: Hypocrisy language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69245b35",
   "metadata": {
    "execution": {}
   },
   "source": [
    "[Falkenberg et al.](https://www.nature.com/articles/s41558-022-01533-z) investigated the hypothesis that *public sentiment* around the COP conferences has increasingly framed them as hypocritical (\"political hypocrisy as a topic of cross-ideological appeal\"). The authors operationalized hypocrisy talk as any tweet containing any of the following words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4a7f67",
   "metadata": {
    "execution": {},
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1682441245326,
     "user": {
      "displayName": "Maximilian Puelma Touzel",
      "userId": "09308600515315501700"
     },
     "user_tz": 240
    }
   },
   "outputs": [],
   "source": [
    "hypocrisy_words = ['hypocrisy', 'hypocrite', 'hypocritical', 'greenwash','green wash', 'blah'] #the last 3 words don't add much. Greta Thurnberg's 'blah, blah blah' speech on Sept. 28th 2021."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c8c539",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The authors then searched for these words within a distinct dataset across all COP conferences (that they did not make openly accessible; result: https://www.nature.com/articles/s41558-022-01527-x/figures/7). The results show that hypocrisy has been mentioned more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DC-Q_Irg9B6O",
   "metadata": {
    "execution": {}
   },
   "source": [
    "How might this matching procedure be limited in its ability to capture this sentiment?\n",
    "\n",
    "Here, we will instead focus on their acessible COP26 dataset and assess the nature of comments about specific topics, like political hypocrisy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf08c45",
   "metadata": {
    "execution": {},
    "executionInfo": {
     "elapsed": 52374,
     "status": "ok",
     "timestamp": 1682441297695,
     "user": {
      "displayName": "Maximilian Puelma Touzel",
      "userId": "09308600515315501700"
     },
     "user_tz": 240
    }
   },
   "outputs": [],
   "source": [
    "hypowords_detector = re.compile(r'\\b(?:{0})\\b'.format('|'.join(hypocrisy_words))) #compile for speed!\n",
    "df['hypotalk']=df.text.apply(lambda x: hypowords_detector.search(x,re.IGNORECASE)) # look through whole dataset, flagging tweets with hypotalk (computes in under a minute)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cbd30c",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Let's pull these tweets out and look at thier occurence statistics relative to those of the whole dataset computed above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3de0fe0",
   "metadata": {
    "execution": {},
    "executionInfo": {
     "elapsed": 1465,
     "status": "ok",
     "timestamp": 1682441299132,
     "user": {
      "displayName": "Maximilian Puelma Touzel",
      "userId": "09308600515315501700"
     },
     "user_tz": 240
    }
   },
   "outputs": [],
   "source": [
    "hyp_tweets=df.loc[~df.hypotalk.isnull(),:]\n",
    "hypo_tweet_counts=hyp_tweets.created_at.groupby(hyp_tweets.created_at.apply(lambda x: x.date)).count().rename('counts')\n",
    "hypo_tweet_fraction=hypo_tweet_counts/total_tweetCounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dynnBOoavDSX",
   "metadata": {
    "execution": {},
    "executionInfo": {
     "elapsed": 1097,
     "status": "ok",
     "timestamp": 1682441300224,
     "user": {
      "displayName": "Maximilian Puelma Touzel",
      "userId": "09308600515315501700"
     },
     "user_tz": 240
    }
   },
   "outputs": [],
   "source": [
    "hypo_tweet_fraction.reset_index().plot(x='created_at',y='counts',figsize=(20,5),style=['.-']) #this doesn't\n",
    "pl.xticks(rotation=45,ha='right');\n",
    "pl.gca().axvspan(*COPdates, alpha=0.3) #gray region\n",
    "pl.gca().set_ylabel('fraction talking about hypocrisy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf23da0",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Note these are normalized so larger fractions closer to the COP26 dates correspond to significantly more absolute number of tweets. \n",
    "\n",
    "Let's look at what these tweets say. Take 100 randomly sampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a6dc14",
   "metadata": {
    "execution": {},
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1682441300225,
     "user": {
      "displayName": "Maximilian Puelma Touzel",
      "userId": "09308600515315501700"
     },
     "user_tz": 240
    }
   },
   "outputs": [],
   "source": [
    "hyp_tweets.text.sample(100).values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8aa791c",
   "metadata": {
    "execution": {}
   },
   "source": [
    "*Exercise*: Pick another sentiment other than 'hypocrisy', and come up with a list of words for it to run the same analysis. E.g. \"renewable technology\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30352a60",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 3: Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ade89e",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The hypocrisy displayed above seems charged against international organizations central to globalization (e.g. G7). Let's assess that hypothesis by analyzing the sentiment of the subset of these hypocrisy tweets that mention these institutions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f87f57",
   "metadata": {
    "execution": {}
   },
   "source": [
    "(part of the computation flow in what follows was taken from Caren Neal's tutorial: https://nealcaren.org/lessons/wordlists/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11be60bb",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We'll match words in tweets to sentiment scores using a dictionary method. The particular dictionary we will use is compiled in the AFINN package. Let's initialize it for the language we've chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7262921",
   "metadata": {
    "execution": {},
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1682441300225,
     "user": {
      "displayName": "Maximilian Puelma Touzel",
      "userId": "09308600515315501700"
     },
     "user_tz": 240
    }
   },
   "outputs": [],
   "source": [
    "afinn = Afinn(language=language_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4b56d8",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Load the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdadf1b6",
   "metadata": {
    "execution": {},
    "executionInfo": {
     "elapsed": 194,
     "status": "ok",
     "timestamp": 1682441300413,
     "user": {
      "displayName": "Maximilian Puelma Touzel",
      "userId": "09308600515315501700"
     },
     "user_tz": 240
    }
   },
   "outputs": [],
   "source": [
    "afinn_wl_url = ('https://raw.githubusercontent.com'\n",
    "                '/fnielsen/afinn/master/afinn/data/AFINN-111.txt')\n",
    "afinn_wl_df = pd.read_csv(afinn_wl_url,\n",
    "                          header=None, # no column names\n",
    "                          sep='\\t',  # tab sepeated\n",
    "                          names=['term', 'value']) #new column names\n",
    "seed = 808 # seed for sample so results are stable\n",
    "afinn_wl_df.sample(10, random_state = seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665cf434",
   "metadata": {
    "execution": {}
   },
   "source": [
    "let's look at the distribution of scores over all words in teh dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0134fb",
   "metadata": {
    "execution": {},
    "executionInfo": {
     "elapsed": 758,
     "status": "ok",
     "timestamp": 1682441301158,
     "user": {
      "displayName": "Maximilian Puelma Touzel",
      "userId": "09308600515315501700"
     },
     "user_tz": 240
    }
   },
   "outputs": [],
   "source": [
    "afinn_wl_df.value.value_counts().sort_index().plot.bar()\n",
    "pl.gca().set_xlabel('Finn score')\n",
    "pl.gca().set_ylabel('dictionary counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecebdb4",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Before focussing on sentiments about institutions within the hypocrisy tweets, let's look at the hypocrisy tweets in comparison to non-hypocrisy tweets. This will take some more intensive computation, so let's only perform it on a 1% subsample of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AKou3UjdVrxi",
   "metadata": {
    "execution": {},
    "executionInfo": {
     "elapsed": 470,
     "status": "ok",
     "timestamp": 1682441301624,
     "user": {
      "displayName": "Maximilian Puelma Touzel",
      "userId": "09308600515315501700"
     },
     "user_tz": 240
    }
   },
   "outputs": [],
   "source": [
    "smalldf=df.sample(frac=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0c8e54",
   "metadata": {
    "execution": {},
    "executionInfo": {
     "elapsed": 82839,
     "status": "ok",
     "timestamp": 1682441384459,
     "user": {
      "displayName": "Maximilian Puelma Touzel",
      "userId": "09308600515315501700"
     },
     "user_tz": 240
    }
   },
   "outputs": [],
   "source": [
    "smalldf['afinn_score'] = smalldf.text.apply(afinn.score) #intensive computation! 1 hrs 50 min long on full dataset! 1% takes about a minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614c3c61",
   "metadata": {
    "execution": {},
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1682441384460,
     "user": {
      "displayName": "Maximilian Puelma Touzel",
      "userId": "09308600515315501700"
     },
     "user_tz": 240
    }
   },
   "outputs": [],
   "source": [
    "smalldf['afinn_score'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb68c87",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The score is a sum over all words in the tweet so to compare we should convert to a per-word score by normalizing each tweet's score by its word count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276d045e",
   "metadata": {
    "execution": {},
    "executionInfo": {
     "elapsed": 328,
     "status": "ok",
     "timestamp": 1682441384757,
     "user": {
      "displayName": "Maximilian Puelma Touzel",
      "userId": "09308600515315501700"
     },
     "user_tz": 240
    }
   },
   "outputs": [],
   "source": [
    "def word_count(text_string):\n",
    "    '''Calculate the number of words in a string'''\n",
    "    return len(text_string.split())\n",
    "smalldf['word_count']=smalldf.text.apply(word_count)\n",
    "smalldf['afinn_adjusted'] = smalldf['afinn_score'] / smalldf['word_count'] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf46a307",
   "metadata": {
    "execution": {},
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1682441384757,
     "user": {
      "displayName": "Maximilian Puelma Touzel",
      "userId": "09308600515315501700"
     },
     "user_tz": 240
    }
   },
   "outputs": [],
   "source": [
    "smalldf['afinn_adjusted'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30f2ef3",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now let's look at the sentiment of tweets with hypocrisy words versus those without those words. We'll first plot for some other possibly negative words for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0f1052",
   "metadata": {
    "execution": {},
    "executionInfo": {
     "elapsed": 1841,
     "status": "ok",
     "timestamp": 1682441386595,
     "user": {
      "displayName": "Maximilian Puelma Touzel",
      "userId": "09308600515315501700"
     },
     "user_tz": 240
    }
   },
   "outputs": [],
   "source": [
    "for sel_words in [['Fossil'],['G7'],['Boris'],['Davos'],hypocrisy_words]:\n",
    "    sel_name = sel_words[0] if len(sel_words)==1 else 'hypowords'\n",
    "    hypowords_detector = re.compile(r'\\b(?:{0})\\b'.format('|'.join(sel_words))) #compile for speed!\n",
    "    smalldf[sel_name] = smalldf.text.apply(lambda x: hypowords_detector.search(x,re.IGNORECASE) is not None) #flag if tweet has word(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PfhXOVpUPbKb",
   "metadata": {
    "execution": {}
   },
   "source": [
    "make cumulative distribtion plots of score distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403538c4",
   "metadata": {
    "execution": {},
    "executionInfo": {
     "elapsed": 2180,
     "status": "ok",
     "timestamp": 1682441388771,
     "user": {
      "displayName": "Maximilian Puelma Touzel",
      "userId": "09308600515315501700"
     },
     "user_tz": 240
    }
   },
   "outputs": [],
   "source": [
    "for sel_words in [['Fossil'],['G7'],['Boris'],['Davos'],hypocrisy_words]:\n",
    "    sel_name =sel_words[0] if len(sel_words)==1 else 'hypowords'\n",
    "    fig,ax=pl.subplots()\n",
    "    pl.gca().set_xlim(-100,100)\n",
    "    pl.gca().set_xlabel('adjusted Finn score')\n",
    "    pl.gca().set_ylabel('probabilty')\n",
    "    counts,bins=np.histogram(smalldf.loc[smalldf[sel_name],'afinn_adjusted'],bins=np.linspace(-100,100,101),density=True)\n",
    "    pl.plot(bins[:-1],np.cumsum(counts),color='C0',label=sel_name+' tweets')\n",
    "    counts,bins=np.histogram(smalldf.loc[~smalldf[sel_name],'afinn_adjusted'],bins=np.linspace(-100,100,101),density=True)\n",
    "    pl.plot(bins[:-1],np.cumsum(counts),color='C1',label='non-'+sel_name+' tweets')\n",
    "    pl.gca().axvline(0,color=[0.7]*3,zorder=1)\n",
    "    pl.legend()\n",
    "    pl.title('cumulative Finn score distribution for '+sel_name+ ' occurence')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d138d86d",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Observe that tweets containing the hypocrisy word set are scored much more negative than all of these. So what is the content of these negative tweets?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7c32ee",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 4: word clouds\n",
    "\n",
    "To analyze word usage, let's first vectorize the text data. Vectorization here means giving each word in the vocabulary an index and transforming each word sequence in each to its vector representation whose sequence of elements are the sequence of the corresponding word indices (e.g. the response `['I','love','icecream']` maps to something like `[34823,5937,79345]`). \n",
    "\n",
    "We'll use and compare two methods. Let's write down what they compute by denoting the index, $d$, over the $D$ documents and the index, $w$, over the $W$ words in the vocabulary (the list of all the words found in all the tweets, which we'll call documents): \n",
    "- term frequency, $\\mathrm{tf}(w,d)$. The frequency of a word $w$ in a document $d$ is $$\\mathrm{tf}(w,d):=\\frac{n(w,d)}{n(d)},$$ where $n(w,d)$ is the number of times term $w$ is in document $d$ and $n(d)=\\sum_{w=1}^W n(w,d)$ is the total number of words in document $d$. The term frequency over all the documents is then, $$\\mathrm{tf}(w):=\\frac{\\sum_{d=1}^D n(d)\\mathrm{tf}(w,d)}{N},$$ where the denominator $N=\\sum_{d=1}^D n(d)$ is just the total word count across all documents.\n",
    "- term frequency-inverse document frequency, $\\mathrm{Tfidf}(w,d):=\\mathrm{tf}(w,d)\\mathrm{idf}(w)$. Here, $$\\mathrm{idf}(w)=\\frac{\\log(D+1)}{\\log(n(w)+1)+1},$$ where $n(w)$ is the number of documents in which term $t$ appears, i.e. $n(w,d)>0$. Idf is like an inverse document frequency. The `sklearn` package then uses $$\\mathrm{Tfidf}(w)=\\frac{1}{D}\\sum_{d=1}^D \\frac{\\mathrm{Tfidf}(w,d)}{||\\mathrm{Tfidf}(\\cdot,d)||},$$ where $||\\vec{x}||=\\sqrt{\\sum_{i=1}^Nx^2_i}$ is the Euclidean norm.\n",
    "\n",
    "$\\mathrm{Tfidf}$ aims to improve frequency as a word relevance metric by downweighting words that appear in many documents since these common words are less discriminative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c053030",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Let's run both these methods and store the vectorized data in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aA8eLUo1ySzJ",
   "metadata": {
    "execution": {},
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1682441388772,
     "user": {
      "displayName": "Maximilian Puelma Touzel",
      "userId": "09308600515315501700"
     },
     "user_tz": 240
    }
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcffa1c",
   "metadata": {
    "execution": {},
    "executionInfo": {
     "elapsed": 241,
     "status": "ok",
     "timestamp": 1682444744215,
     "user": {
      "displayName": "Maximilian Puelma Touzel",
      "userId": "09308600515315501700"
     },
     "user_tz": 240
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "vectypes=['counts','Tfidf']\n",
    "def vectorize(doc_data,ngram_range=(1,1),remove_words=[],min_doc_freq=1):\n",
    "\n",
    "  vectorized_data_dict={}\n",
    "  for vectorizer_type in vectypes:\n",
    "      if vectorizer_type=='counts':\n",
    "          vectorizer = CountVectorizer(stop_words=remove_words,min_df=min_doc_freq,ngram_range=ngram_range)\n",
    "      elif vectorizer_type=='Tfidf':\n",
    "          vectorizer = TfidfVectorizer(stop_words=remove_words,min_df=min_doc_freq,ngram_range=ngram_range)\n",
    "\n",
    "      vectorized_doc_list = vectorizer.fit_transform(data).todense().tolist()\n",
    "      feature_names = vectorizer.get_feature_names_out() # or  get_feature_names() depending on scikit learn version\n",
    "      print('vocabulary size:'+str(len(feature_names)))\n",
    "      wdf = pd.DataFrame(vectorized_doc_list, columns=feature_names)\n",
    "      vectorized_data_dict[vectorizer_type]=wdf\n",
    "  return vectorized_data_dict,feature_names\n",
    "\n",
    "def plot_wordcloud_and_freqdist(wdf,title_str,feature_names):\n",
    "    '''\n",
    "    Plots a word cloud\n",
    "    '''\n",
    "    pixel_size=600\n",
    "    x,y =np.ogrid[:pixel_size,:pixel_size]\n",
    "    mask = (x -pixel_size/2)**2+(y-pixel_size/2)**2>(pixel_size/2-20)**2\n",
    "    mask=255*mask.astype(int)\n",
    "    wc=WordCloud(background_color=\"rgba(255, 255, 255, 0)\", mode=\"RGBA\", mask=mask, max_words=50)#,relative_scaling=1)\n",
    "    wordfreqs=wdf.T.sum(axis=1)\n",
    "    num_show=50\n",
    "    sorted_ids=np.argsort(wordfreqs)[::-1]\n",
    "\n",
    "    fig,ax=pl.subplots(figsize=(10,5))\n",
    "    ax.bar(x=range(num_show),height=wordfreqs[sorted_ids][:num_show])\n",
    "    ax.set_xticks(range(num_show))\n",
    "    ax.set_xticklabels(feature_names[sorted_ids][:num_show],rotation=45,fontsize=8,ha='right');\n",
    "    ax.set_ylabel('total frequency')\n",
    "    ax.set_title(title_str+' vectorizer')\n",
    "    ax.set_ylim(0,10*wordfreqs[sorted_ids][int(num_show/2)])\n",
    "\n",
    "    ax_wc = inset_axes(ax,width='90%',height='90%')\n",
    "    wc.generate_from_frequencies(wordfreqs)\n",
    "    ax_wc.imshow(wc,interpolation='bilinear')\n",
    "    ax_wc.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b007814",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Okay so let's vectorize and look at the wordclouds for single word statistics. Let's explicitly exclude some words and implicity exclude ones that appear in fewer than some threshold number of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DAgLmKhVf6YH",
   "metadata": {
    "execution": {},
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1682444734903,
     "user": {
      "displayName": "Maximilian Puelma Touzel",
      "userId": "09308600515315501700"
     },
     "user_tz": 240
    }
   },
   "outputs": [],
   "source": [
    "data=hyp_tweets['text'].sample(frac=0.1).values #reduce size since this cell produces arrays of size: vocabulary size x number of tweets\n",
    "\n",
    "remove_words = stopwords.words(\"english\") #stop words are words will little semantic value (e.g. \"the\")\n",
    "#let's add some mroe words we don't want to track (you can generate this kind of list iteratively by looking at the results):\n",
    "remove_words += ['cop26','http','https','30','000','je','rt','climate','limacop20','un_climatetalks','climatechange','via','ht','talks','unfccc','peru','peruvian','lima','co']\n",
    "print(str(len(data))+\" tweets\")\n",
    "min_doc_freq=5/len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1884136",
   "metadata": {
    "execution": {},
    "executionInfo": {
     "elapsed": 12260,
     "status": "ok",
     "timestamp": 1682444758286,
     "user": {
      "displayName": "Maximilian Puelma Touzel",
      "userId": "09308600515315501700"
     },
     "user_tz": 240
    }
   },
   "outputs": [],
   "source": [
    "ngram_range=(1,1) #start and end number of words\n",
    "vectorized_data_dict,feature_names=vectorize(hyp_tweets,ngram_range=ngram_range,remove_words=remove_words,min_doc_freq=min_doc_freq)\n",
    "for vectorizer_type in vectypes:\n",
    "    plot_wordcloud_and_freqdist(vectorized_data_dict[vectorizer_type],vectorizer_type,feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3d679a",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Note how the TFidf vectorizer has scaled down the hypocrisy words relative to the count vectorizer. \n",
    "\n",
    "There are some words here that look like they likely would appear in pairs. Let's tell the vectorizer to also look for high frequency *pairs* of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde8ddf7",
   "metadata": {
    "execution": {},
    "executionInfo": {
     "elapsed": 30399,
     "status": "ok",
     "timestamp": 1682444823790,
     "user": {
      "displayName": "Maximilian Puelma Touzel",
      "userId": "09308600515315501700"
     },
     "user_tz": 240
    }
   },
   "outputs": [],
   "source": [
    "ngram_range= (1,2)#start and end number of words\n",
    "vectorized_data_dict,feature_names=vectorize(hyp_tweets,ngram_range=ngram_range,remove_words=remove_words,min_doc_freq=min_doc_freq)\n",
    "for vectorizer_type in vectypes:\n",
    "    plot_wordcloud_and_freqdist(vectorized_data_dict[vectorizer_type],vectorizer_type,feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4fe0fb",
   "metadata": {
    "execution": {}
   },
   "source": [
    "To clear this list a bit more, let's also remove the hypocrisy words altogether.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MX9q2k-WiM1D",
   "metadata": {
    "execution": {},
    "executionInfo": {
     "elapsed": 153,
     "status": "ok",
     "timestamp": 1682444966342,
     "user": {
      "displayName": "Maximilian Puelma Touzel",
      "userId": "09308600515315501700"
     },
     "user_tz": 240
    }
   },
   "outputs": [],
   "source": [
    "remove_words += hypocrisy_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZyAs5RMXiADq",
   "metadata": {
    "execution": {},
    "executionInfo": {
     "elapsed": 25360,
     "status": "ok",
     "timestamp": 1682444992874,
     "user": {
      "displayName": "Maximilian Puelma Touzel",
      "userId": "09308600515315501700"
     },
     "user_tz": 240
    }
   },
   "outputs": [],
   "source": [
    "ngram_range= (1,2)#start and end number of words\n",
    "vectorized_data_dict,feature_names=vectorize(hyp_tweets,ngram_range=ngram_range,remove_words=remove_words,min_doc_freq=min_doc_freq)\n",
    "for vectorizer_type in vectypes:\n",
    "    plot_wordcloud_and_freqdist(vectorized_data_dict[vectorizer_type],vectorizer_type,feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fsgxp-SWh_iO",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Observe terms that we might have expected are associated to hypocrisy, e.g. \"flying\". Even when allowing for pairs, the semantics are hard to extract from this analysis that ignores the correlations in usage among multiple words. \n",
    "\n",
    "To futher assess statistics, one approach is use a generative model with latent structure.\n",
    "\n",
    "Topic models (the [structural topic model](https://www.structuraltopicmodel.com/) in particular) are a nice modelling framework to start analyzing those correlations."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "W2D3_Tutorial4",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (pangeo)",
   "language": "python",
   "name": "pangeo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}