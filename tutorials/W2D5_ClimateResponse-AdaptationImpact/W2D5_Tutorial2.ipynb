{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f52dc563",
   "metadata": {
    "execution": {}
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ClimateMatchAcademy/course-content/blob/main/tutorials/W2D5_ClimateResponse-AdaptationImpact/W2D5_Tutorial2.ipynb) &nbsp; <a href=\"https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/ClimateMatchAcademy/course-content/main/tutorials/W2D5_ClimateResponse-AdaptationImpact/W2D5_Tutorial2.ipynb\" target=\"_parent\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open in Kaggle\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "myt07YFyNgmw",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Tutorial 2:  Regression and Decision Trees on the Dengue Fever Dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Week 2, Day 5: Adaptation and Impact**\n",
    "\n",
    "__Content creators:__ Deepak Mewada, Grace Lindsay\n",
    "\n",
    "__Content reviewers:__ Dionessa Biton, Younkap Nina Duplex, Sloane Garelick, Zahra Khodakaramimaghsoud, Peter Ohue, Jenna Pearson, Derick Temfack, Peizhen Yang, Cheng Zhang, Chi Zhang, Ohad Zivan\n",
    "\n",
    "__Content editors:__ Jenna Pearson, Chi Zhang, Ohad Zivan\n",
    "\n",
    "__Production editors:__ Wesley Banfield, Jenna Pearson, Chi Zhang, Ohad Zivan\n",
    "\n",
    "**Our 2023 Sponsors:** NASA TOPS and Google DeepMind"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kDQc1jnoNWcp",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Tutorial Objectives\n",
    "\n",
    "Welcome to tutorial 2 of a series focused on understanding the role of data science and machine learning in addressing the impact of climate change and adapting to it.\n",
    "\n",
    "In this tutorial, we will explore a dataset that relates weather variables to dengue fever cases. By the end of the tutorial, you will be able to:\n",
    "\n",
    "- Load the data into pandas dataframes and visualize it to see obvious trends.\n",
    "- Apply linear regression to the Dengue dataset: clean the data, implement linear regression using scikit-learn, and evaluate its performance. This will include handling categorical data using dummy variables and using scikit-learn's Poisson GLM method to handle integer-valued data.\n",
    "- Apply additional methods to the Dengue Fever dataset, including implementing Random Forest Regression, discussing and analyzing its performance, and measuring feature importance.\n",
    "\n",
    "This tutorial will provide you with practical experience in working with real-world datasets and implementing different regression techniques.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Vtq0OyoRNPcc",
   "metadata": {
    "execution": {}
   },
   "source": [
    "\n",
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Kwsl6-KNNPcc",
   "metadata": {
    "execution": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import numpy as np  # import the numpy library as np\n",
    "from sklearn.linear_model import (\n",
    "    LinearRegression,\n",
    ")  # import the LinearRegression class from the sklearn.linear_model module\n",
    "import matplotlib.pyplot as plt  # import the pyplot module from the matplotlib library\n",
    "import pandas as pd  # import the pandas library and the drive function from the google.colab module\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.linear_model import PoissonRegressor\n",
    "\n",
    "from matplotlib import cm\n",
    "import pooch\n",
    "import os\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2438d815",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Figure Settings\n",
    "import ipywidgets as widgets  # interactive display\n",
    "\n",
    "plt.style.use(\n",
    "    \"https://raw.githubusercontent.com/ClimateMatchAcademy/course-content/main/cma.mplstyle\"\n",
    ")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1aa501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Helper functions\n",
    "\n",
    "\n",
    "def pooch_load(filelocation=None, filename=None, processor=None):\n",
    "    shared_location = \"/home/jovyan/shared/Data/tutorials/W2D5_ClimateResponse-AdaptationImpact\"  # this is different for each day\n",
    "    user_temp_cache = tempfile.gettempdir()\n",
    "\n",
    "    if os.path.exists(os.path.join(shared_location, filename)):\n",
    "        file = os.path.join(shared_location, filename)\n",
    "    else:\n",
    "        file = pooch.retrieve(\n",
    "            filelocation,\n",
    "            known_hash=None,\n",
    "            fname=os.path.join(user_temp_cache, filename),\n",
    "            processor=processor,\n",
    "        )\n",
    "\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531684ac-2ca0-4444-b5bd-9cd1e48fffa6",
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @title Video 1: Dengue Fever Dataset\n",
    "\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import YouTubeVideo\n",
    "from IPython.display import IFrame\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "class PlayVideo(IFrame):\n",
    "  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n",
    "    self.id = id\n",
    "    if source == 'Bilibili':\n",
    "      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n",
    "    elif source == 'Osf':\n",
    "      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n",
    "    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "\n",
    "def display_videos(video_ids, W=400, H=300, fs=1):\n",
    "  tab_contents = []\n",
    "  for i, video_id in enumerate(video_ids):\n",
    "    out = widgets.Output()\n",
    "    with out:\n",
    "      if video_ids[i][0] == 'Youtube':\n",
    "        video = YouTubeVideo(id=video_ids[i][1], width=W,\n",
    "                             height=H, fs=fs, rel=0)\n",
    "        print(f'Video available at https://youtube.com/watch?v={video.id}')\n",
    "      else:\n",
    "        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n",
    "                          height=H, fs=fs, autoplay=False)\n",
    "        if video_ids[i][0] == 'Bilibili':\n",
    "          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n",
    "        elif video_ids[i][0] == 'Osf':\n",
    "          print(f'Video available at https://osf.io/{video.id}')\n",
    "      display(video)\n",
    "    tab_contents.append(out)\n",
    "  return tab_contents\n",
    "\n",
    "\n",
    "video_ids = [('Youtube', '6k4w2gBkvrw'), ('Bilibili', 'BV1LV4y1b7gH')]\n",
    "tab_contents = display_videos(video_ids, W=730, H=410)\n",
    "tabs = widgets.Tab()\n",
    "tabs.children = tab_contents\n",
    "for i in range(len(tab_contents)):\n",
    "  tabs.set_title(i, video_ids[i][0])\n",
    "display(tabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d2a276-67fb-4cf7-b647-9218dda9c0f8",
   "metadata": {
    "cellView": "form",
    "execution": {},
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @markdown\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import IFrame\n",
    "\n",
    "link_id = \"2jh94\"\n",
    "\n",
    "download_link = f\"https://osf.io/download/{link_id}/\"\n",
    "render_link = f\"https://mfr.ca-1.osf.io/render?url=https://osf.io/{link_id}/?direct%26mode=render%26action=download%26mode=render\"\n",
    "# @markdown\n",
    "out = widgets.Output()\n",
    "with out:\n",
    "    print(f\"If you want to download the slides: {download_link}\")\n",
    "    display(IFrame(src=f\"{render_link}\", width=730, height=410))\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nULavCfq4o07",
   "metadata": {
    "execution": {}
   },
   "source": [
    "\n",
    "\n",
    "# Section 1: Loading and Exploring Dengue Fever Data Set\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j9n17gXFOIXX",
   "metadata": {
    "execution": {}
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "## Section 1.1:  Loading the Environmental Data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c27233",
   "metadata": {
    "execution": {}
   },
   "source": [
    "As discussed in the video, we are working with a [data set](https://www.drivendata.org/competitions/44/dengai-predicting-disease-spread/page/81/) provided by [DrivenData](https://arxiv.org/abs/1606.07781) that centers on the goal of predicting dengue fever cases based on environmental variables.\n",
    "\n",
    "We will use pandas to interface with the data, which is shared in the .csv format. First, let's load the environmental data into a pandas dataframe and print its contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hpeaOl-v3Mod",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# loading a CSV file named 'dengue_features_train(1).csv' into a pandas DataFrame named 'df'\n",
    "filename_dengue = \"dengue_features_train.csv\"\n",
    "url_dengue = \"https://osf.io/wm9un/download/\"\n",
    "\n",
    "df_features = pd.read_csv(pooch_load(url_dengue, filename_dengue))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dOsBX8X6ScK",
   "metadata": {
    "execution": {}
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "## Section 1.2:  Explore the Dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Vv82LGM26hiD",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# displaying the contents of the DataFrame named 'df'.\n",
    "# this is useful for verifying that the data was read correctly.\n",
    "df_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e345ba",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We can see some of the variables discussed in the video. For full documentation of these features, see the associated description [here](https://www.drivendata.org/competitions/44/dengai-predicting-disease-spread/page/82/). \n",
    "\n",
    "In climate science, visualizing data is an important step in understanding patterns and trends in environmental data. It can also help identify outliers and inconsistencies that need to be addressed before modeling. Therefore, in the next subsection, we will visualize the climate and environmental data to gain insights and identify potential issues.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98JK1Hy3IN0P",
   "metadata": {
    "execution": {}
   },
   "source": [
    "\n",
    "## Section 1.3: Visualize the Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0Q1GTTR8imTs",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Coding Exercise 1.1\n",
    "\n",
    "For this exercise, you will visualize the data. Use the provided hints for the function name if needed.\n",
    "\n",
    "1. Use pandas to plot histograms of these features using .hist() function\n",
    "2. Use the .isnull() function to see if there is any missing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lawwAEiC8vZV",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# display a histogram of the Pandas DataFrame 'df'\n",
    "# df_features.hist(...)\n",
    "\n",
    "# output the sum of null values for each column in 'df'\n",
    "df_features.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EQuT_FPO4T6P",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "\n",
    "# display a histogram of the Pandas DataFrame 'df'\n",
    "# df_features.hist(figsize=[20,20])\n",
    "\n",
    "# output the sum of null values for each column in 'df'\n",
    "df_features.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bdc6b4",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The above histograms show the number of data points that have the value given on the x-axis. As we can see, there is some missing data as well, particularly some of the Normalized Difference Vegetation Index (NDVI) measures. So far we have only been looking at the environmental data. Let's load the corresponding weekly dengue fever cases as well. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FqcYp2yRhOAc",
   "metadata": {
    "execution": {}
   },
   "source": [
    "\n",
    "## Section 1.4: Load and Visualize the Corresponding Weekly Dengue Fever Cases\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9uCmTha48dY-",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# reading a CSV file named \"dengue_labels_train.csv\" and saving the data into a pandas DataFrame named \"df_dengue\".\n",
    "filename_dengue_labels = \"dengue_labels_train.csv\"\n",
    "url_dengue_labels = \"https://osf.io/6nw9x/download\"\n",
    "\n",
    "df_labels = pd.read_csv(pooch_load(url_dengue_labels, filename_dengue_labels))\n",
    "\n",
    "# displaying the contents of the DataFrame named \"df_dengue\".\n",
    "# This is useful for verifying that the data was read correctly.\n",
    "df_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6Dq4k8mI4fO",
   "metadata": {
    "execution": {}
   },
   "source": [
    "\n",
    "## Section 1.5: Visualize the Weekly Dengue Fever Cases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1M2cMCvkgyiD",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Coding Exercises 1.5\n",
    "\n",
    "For the following exercises visualise the data. You will start with plotting a histogram of the case numbers.\n",
    "  \n",
    "1. Plot a histogram of the case numbers using 30 bins (bins = 30)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Qr3PaKJKm7yL",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# display a histogram of the 'total_cases' column in the 'df_dengue' DataFrame\n",
    "_ = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vGdyLzTOYNsX",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "\n",
    "# display a histogram of the 'total_cases' column in the 'df_dengue' DataFrame\n",
    "_ = df_labels.hist(column=\"total_cases\", bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GQgRG63VkQIM",
   "metadata": {
    "execution": {}
   },
   "source": [
    "2. Plot total cases as a function of year\n",
    "2. Plot total cases as a function of the week of the year\n",
    "3. Plot total cases as two separate histograms, one for each of the cities with 30 bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RsdGCbDw-NlG",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# creating a scatter plot of 'year' vs 'total_cases' in the DataFrame 'df_dengue'.\n",
    "_ = ...\n",
    "\n",
    "# creating a scatter plot of 'weekofyear' vs 'total_cases' in the DataFrame 'df_dengue'.\n",
    "_ = ...\n",
    "\n",
    "# creating a new DataFrame named 'new' that contains only the columns 'total_cases' and 'city' from 'df_dengue'.\n",
    "new = ...\n",
    "\n",
    "# creating histograms of the 'total_cases' column in 'new' separated by the values in the 'city' column.\n",
    "new = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aC3XX1S8-Din",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "\n",
    "# creating a scatter plot of 'year' vs 'total_cases' in the DataFrame 'df_dengue'.\n",
    "_ = df_labels.plot.scatter(\"year\", \"total_cases\")\n",
    "\n",
    "# creating a scatter plot of 'weekofyear' vs 'total_cases' in the DataFrame 'df_dengue'.\n",
    "_ = df_labels.plot.scatter(\"weekofyear\", \"total_cases\")\n",
    "\n",
    "# creating a new DataFrame named 'new' that contains only the columns 'total_cases' and 'city' from 'df_dengue'.\n",
    "new = df_labels[[\"total_cases\", \"city\"]].copy()\n",
    "\n",
    "# creating histograms of the 'total_cases' column in 'new' separated by the values in the 'city' column.\n",
    "new.hist(by=\"city\", bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ucPP9sGWo0Nc",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Questions 1.5\n",
    "\n",
    "1. Delve deeper into the data. What trends do you observe?\n",
    "Engage in a discussion with your group and see if anyone has noticed a trend that you may have missed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f9be1c",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove explanation\n",
    "\"\"\"\n",
    "1. Possible observations: In the top figure there are some years with anomously high weeks of case numbers such as 1994, 1998, and 2006. In the second figure it appears weeks with the high case numbers are consistent across years, with higher numbers appears for week of year greater than 20. In the bottom figure, the cities have comparable distribution shapes however the number of cases per week and frequency in San Juan is much higher than Iquitos.\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OJfHILMlkRtg",
   "metadata": {
    "execution": {}
   },
   "source": [
    "\n",
    "# Section 2 : Regression on Dengue Fever Dataset\n",
    "\n",
    "In the previous section, we explored the Dengue dataset. In this section, we will apply linear regression to this dataset.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "V2qPY6gYuetd",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Since we have already explored and visualized the dataset we are ready to train a model.\n",
    "\n",
    "We will start by preprocessing the data and defining a model to predict cases of dengue fever.\n",
    "\n",
    "In the case of predicting dengue fever the environmental variables are the independent variables (or regressors), while number of dengue fever cases is the dependent variable that we want to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "erqZu0VckSJc",
   "metadata": {
    "execution": {}
   },
   "source": [
    "\n",
    "## Section 2.1: Data Preprocessing\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SYvfpa8B8S_D",
   "metadata": {
    "execution": {}
   },
   "source": [
    "\n",
    "In climate science, data is often incomplete or missing, and it's important to handle such missing values before using the data for prediction or analysis. There are [many ways](https://towardsdatascience.com/7-ways-to-handle-missing-values-in-machine-learning-1a6326adf79e) to do this. In our case, we will remove the missing values as well as columns of data we won't be using in the predictive model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qCBHC6U78sL3",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# data cleaning\n",
    "\n",
    "# drop columns 'city', 'year', and 'week_start_date' from the 'df' dataframe to create a new dataframe 'df_cleaned'\n",
    "df_cleaned = df_features.drop([\"city\", \"year\", \"week_start_date\"], axis=1)\n",
    "\n",
    "# check for null and missing values\n",
    "print(\"Null and missing value before cleaning\", df_cleaned.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YrRfK2T8MW-m",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# remove missing values (NaN). there are works in the scientific literature on how to fill the missing gaps.\n",
    "# the safest method is to just remove the missing values for now.\n",
    "df_cleaned = df_cleaned.dropna()\n",
    "\n",
    "# check for null and missing values after removing\n",
    "print(df_cleaned.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SvS4eEF1f6uL",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0ce4ad",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Notice that the number of rows in the dataset has decreased from 1456 to 1199. We need to ensure that the dataset with dengue cases has the same rows dropped as we just did above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60870653",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# use loc to select the relevant indexes\n",
    "df_labels = df_labels.loc[df_cleaned.index]\n",
    "df_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Rauj0P6EbNmz",
   "metadata": {
    "execution": {}
   },
   "source": [
    "To build a model with the climate data, we need to divide it into a training and test set to ensure that the model works well on held-out data. This is important because, as we have seen, evaluating the model on the exact data it was trained on can be misleading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "M0XopmfSu7Ek",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# train-test split\n",
    "\n",
    "# select the 'total_cases' column from the 'df_dengue' dataframe and assign it to the variable 'cases'\n",
    "cases = df_labels[\"total_cases\"]\n",
    "\n",
    "# create a boolean mask with random values for each element in 'cases'\n",
    "np.random.seed(\n",
    "    145\n",
    ")  # setting the random seed ensures we are all using the same train/test split\n",
    "mask = (\n",
    "    np.random.rand(len(cases)) < 0.8\n",
    ")  # this will use 80% of the data to train and 20% to test\n",
    "\n",
    "# create two new dataframes from the 'df_cleaned' dataframe based on the boolean mask\n",
    "df_cleaned_train = df_cleaned[mask]\n",
    "df_cleaned_test = df_cleaned[~mask]\n",
    "\n",
    "# create two new arrays from the 'cases' array based on the boolean mask\n",
    "cases_train = cases[mask]\n",
    "cases_test = cases[~mask]\n",
    "\n",
    "print(\n",
    "    \"80% of the data is split into the training set and remaining 20% into the test set.\"\n",
    ")\n",
    "\n",
    "# check that this is true\n",
    "print(\"length of training data: \", df_cleaned_train.shape[0])\n",
    "print(\"length of test data: \", df_cleaned_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zPeztNPTwRKx",
   "metadata": {
    "execution": {}
   },
   "source": [
    "\n",
    "\n",
    "## Section 2.2: Fitting Model and Analyzing Results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kw987L7PkSJd",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Coding Exercise 2.2: Implement Regression on Dengue Fever Dataset and Evaluate the Performance\n",
    "For this exercise, use what you learned in the previous tutorials to train a linear regression model on the training data and evaluate its performance. Evaluate its performance on the training data and the test data. Look specifically at the difference between predicted values and true values on the test set. \n",
    "\n",
    "1. Train a linear regression model on the training data\n",
    "2. Evaluate the performance of the model on both training and test data.\n",
    "3. Look specifically at the difference between predicted values and true values on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2boskgTN8REW",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# create a new instance of the LinearRegression class\n",
    "reg_model = LinearRegression()\n",
    "\n",
    "# train the model on the training data i.e on df_cleaned_train,cases_train\n",
    "_ = ...\n",
    "\n",
    "# print the R^2 score of the trained model on the training data\n",
    "print(\"r^2 on training data is: \")\n",
    "print(...)\n",
    "\n",
    "# print the R^2 score of the trained model on the test data\n",
    "print(\"r^2 on test data is: \")\n",
    "print(...)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "# create a scatter plot of the predicted values vs. the actual values for the test data\n",
    "_ = ...\n",
    "\n",
    "# add 1:1 line\n",
    "ax.plot(np.array(ax.get_xlim()), np.array(ax.get_xlim()), \"k-\")\n",
    "\n",
    "# add axis labels to the scatter plot\n",
    "ax.set_xlabel(\"Actual Number of Dengue Cases\")\n",
    "ax.set_ylabel(\"Predicted Number of Dengue Cases\")\n",
    "ax.set_title(\"Predicted values vs. the actual values for the test data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a621151",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "\n",
    "# create a new instance of the LinearRegression class\n",
    "reg_model = LinearRegression()\n",
    "\n",
    "# train the model on the training data i.e on df_cleaned_train,cases_train\n",
    "reg_model.fit(df_cleaned_train, cases_train)\n",
    "\n",
    "# print the R^2 score of the trained model on the training data\n",
    "print(\"r^2 on training data is: \")\n",
    "print(reg_model.score(df_cleaned_train, cases_train))\n",
    "\n",
    "# print the R^2 score of the trained model on the test data\n",
    "print(\"r^2 on test data is: \")\n",
    "print(reg_model.score(df_cleaned_test, cases_test))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "# create a scatter plot of the predicted values vs. the actual values for the test data\n",
    "_ = ax.scatter(cases_test, reg_model.predict(df_cleaned_test))\n",
    "\n",
    "# add 1:1 line\n",
    "ax.plot(np.array(ax.get_xlim()), np.array(ax.get_xlim()), \"k-\")\n",
    "\n",
    "# add axis labels to the scatter plot\n",
    "ax.set_xlabel(\"Actual Number of Dengue Cases\")\n",
    "ax.set_ylabel(\"Predicted Number of Dengue Cases\")\n",
    "ax.set_title(\"Predicted values vs. the actual values for the test data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eTro7UedNRmk",
   "metadata": {
    "execution": {}
   },
   "source": [
    "<details>\n",
    "<summary> <font color='orangered'>Click here description of plot  </font></summary>\n",
    "\n",
    "This code trains a linear regression model on a dataset and evaluates its performance on both the training and test data.\n",
    "\n",
    "The scatter plot generated at the end of the code shows the predicted values vs. the actual values for the test data. Each point in the scatter plot represents a single data point in the test set. The horizontal axis represents the actual number of dengue cases, while the vertical axis represents the predicted number of dengue cases.\n",
    "\n",
    "If the predicted values are close to the actual values, the scatter plot will show a diagonal line where the points cluster around. On the other hand, if the predicted values are far from the actual values, the scatter plot will be more spread out and may show a more random pattern of points.\n",
    "\n",
    "By visually inspecting the scatter plot and looking at the r^2, we can get an idea of how well the model is performing and how accurate its predictions are. How well does the model do?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752f819d",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Questions 2.2\n",
    "\n",
    "1. Discuss any surprising features of this plot related to model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff7b43c",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove explanation\n",
    "\"\"\"\n",
    "1. Possible discussion: The model cannot predict values above 50 cases per week. This is likely due to an imbalance of data. Referencing the data exploration figures from the first section, we see that most of the data falls at or below 50 cases per week. This type of model does not excel at predicting extreme cases, and this is reflected on the plot above. Additionally, note the negative values (up to -10 cases per week) predicted by our model. It is of course not possible to have negative case numbers, however you can see that the linear regression equation does not prevent negative numbers. Later on in the bonus section you will look at a model more suitable for our dataset type that also guarantees non-negative results.\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22915838",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We can also calculate some other metrics to assess model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BR4APm-zqPO9",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# evaluating the performance of the model using metrics such as mean absolute error (MAE) and mean squared error (MSE)\n",
    "\n",
    "y_pred = reg_model.predict(df_cleaned_test)\n",
    "print(\"MAE:\", mean_absolute_error(cases_test, y_pred))\n",
    "print(\"RMSE:\", mean_squared_error(cases_test, y_pred, squared=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dKZf17NOWd-",
   "metadata": {
    "execution": {}
   },
   "source": [
    "<details>\n",
    "<summary> <font color='violet'>What is MAE and MSE   </font></summary>\n",
    "In addition to r^2, Mean Absolute Error (MAE) and Mean Squared Error (MSE) are both metrics used to evaluate the performance of a regression model. They both measure the difference between the predicted values and the actual values of the target variable.\n",
    "\n",
    "The MAE is calculated by taking the average of the absolute differences between the predicted and actual values:\n",
    "\\begin{align}\n",
    "MAE = \\frac{1}{n}\\sum_{i=1}^{n}|y_i - \\hat{y_i}|\n",
    "\\end{align}\n",
    "where $n$ is the number of samples, $y_i$ is the actual value of the target variable for sample $i$, and $\\hat{y}_i$ is the predicted value of the target variable for sample $i$.\n",
    "\n",
    "The RMSE is calculated by taking the square root of the average of the squared differences between the predicted and actual values:\n",
    "\\begin{align}\n",
    " RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y_i})^2}\n",
    "\\end{align} \n",
    "The main difference between MAE and RMSE is that RMSE gives more weight to larger errors, because the differences are squared. Therefore, if there are some large errors in the predictions, the RMSE will be higher than the MAE.\n",
    "\n",
    "Both MAE and RMSE have the same unit of measurement as the target variable, and lower values indicate better model performance. However, the choice of which metric to use depends on the specific problem and the goals of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1LO0O5YkUsZB",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## (Bonus) Section 2.3 : Handling Different Scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Y6Pux46LWw7e",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Section 2.3.1: Handling Categorical Regressors*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RNS2yeHewz7r",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We chose to remove city as a regressor because it is not a numerical value and therefore does not fit as easily into the linear regression framework. However, it is possible to include such categorical data. To do so, you need to turn the string variables representing cities into 'dummy variables', that is, numerical values that stand in for the categories. Here we can simply arbitrarily set one city to the value 0 and the other the value 1. See how including city impacts regression performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86f64c3",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "df_cleaned_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pIVU36FuB1cc",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# include city as a regressor by creating dummy variables for the 'city' column\n",
    "df_cleaned_city = pd.get_dummies(df_features.dropna()[[\"city\"]], drop_first=True)\n",
    "\n",
    "# combine the cleaned data with the city dummy variables\n",
    "df_cleaned_combined = pd.concat([df_cleaned, df_cleaned_city], axis=1)\n",
    "\n",
    "# split the data into training and test sets using a random mask\n",
    "np.random.seed(145)\n",
    "mask = np.random.rand(len(cases)) < 0.8\n",
    "df_cleaned_city_train = df_cleaned_combined[mask]\n",
    "df_cleaned_city_test = df_cleaned_combined[~mask]\n",
    "cases_city_train = cases[mask]\n",
    "cases_city_test = cases[~mask]\n",
    "\n",
    "# train a linear regression model with city as a regressor\n",
    "reg_model_city = LinearRegression()\n",
    "reg_model_city.fit(df_cleaned_city_train, cases_city_train)\n",
    "\n",
    "# print R-squared scores for the train and test sets\n",
    "print(\n",
    "    \"R-squared on training data is: \",\n",
    "    reg_model_city.score(df_cleaned_city_train, cases_city_train),\n",
    ")\n",
    "print(\n",
    "    \"R-squared on test data is: \",\n",
    "    reg_model_city.score(df_cleaned_city_test, cases_city_test),\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "# create a scatter plot of the predicted values vs. the actual values for the test data\n",
    "ax.scatter(cases_city_test, reg_model_city.predict(df_cleaned_city_test))\n",
    "\n",
    "# add 1:1 line\n",
    "ax.plot(np.array(ax.get_xlim()), np.array(ax.get_xlim()), \"k-\")\n",
    "\n",
    "ax.set_xlabel(\"Actual number of dengue cases\")\n",
    "ax.set_ylabel(\"Predicted number of dengue cases\")\n",
    "ax.set_title(\"Predicted vs Actual number of dengue cases (with city as a regressor)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AwjpUDzNTTwN",
   "metadata": {
    "execution": {}
   },
   "source": [
    "<details>\n",
    "<summary> <font color='orangered'>Click here description of plot  </font></summary>\n",
    "The plot generated is a scatter plot with the actual total cases on the x-axis and the predicted total cases on the y-axis. Each point on the plot represents a test data point. \n",
    "The accuracy of the model can also be evaluated numerically by computing metrics such as mean absolute error (MAE) and mean squared error (MSE), as well as the R-squared score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MaxjVAr7qoZe",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# evaluating the performance of the model using metrics such as mean absolute error (MAE) and root mean squared error (RMSE)\n",
    "\n",
    "y_pred_city = reg_model_city.predict(df_cleaned_city_test)\n",
    "print(\"MAE:\", mean_absolute_error(cases_city_test, y_pred_city))\n",
    "print(\"RMSE:\", mean_squared_error(cases_city_test, y_pred_city, squared=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UJjPu06M-fAG",
   "metadata": {
    "execution": {}
   },
   "source": [
    "###  Section 2.3.2 : Handling Integer Valued Dependent Variables "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7OZbW4DpDtT5",
   "metadata": {
    "execution": {}
   },
   "source": [
    "In our simulated data from the previous tutorial, the dependent variable was real-valued and followed a normal distribution. Here, the weekly case numbers are integers and are better described by a [Poisson distribution](https://en.wikipedia.org/wiki/Poisson_distribution). Therefore, plain linear regression is not actually the most appropriate approach for this data. Rather, we should use a generalized linear model, or GLM, which is like linear regression, but includes an extra step that makes it more suited to handle Poisson data and prevent negative case numbers. Try to use [scikit-learn's Poisson GLM method](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.PoissonRegressor.html) on this data. Evaluate the performance of this model using the same metrics as above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-Ux5wOt-DZS-",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# create PoissonRegressor object\n",
    "poisson_reg = PoissonRegressor()\n",
    "\n",
    "# fit the PoissonRegressor model with training data\n",
    "poisson_reg.fit(df_cleaned_train, cases_train)\n",
    "\n",
    "# calculate r^2 score on training data\n",
    "print(\"r^2 on training data is: \")\n",
    "print(poisson_reg.score(df_cleaned_train, cases_train))\n",
    "\n",
    "# calculate r^2 score on test data\n",
    "print(\"r^2 on test data is: \")\n",
    "print(poisson_reg.score(df_cleaned_test, cases_test))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "# plot predicted values against test data\n",
    "ax.scatter(cases_test, poisson_reg.predict(df_cleaned_test))\n",
    "\n",
    "# add 1:1 line\n",
    "ax.plot(np.array(ax.get_xlim()), np.array(ax.get_xlim()), \"k-\")\n",
    "\n",
    "# add plot title and labels\n",
    "ax.set_title(\"Predicted Cases vs Actual Cases\")\n",
    "ax.set_xlabel(\"Actual Cases\")\n",
    "ax.set_ylabel(\"Predicted Cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZDS3UirhTVhl",
   "metadata": {
    "execution": {}
   },
   "source": [
    "<details>\n",
    "<summary> <font color='orangered'>Click here description of plot  </font></summary>\n",
    "The plot generated is a scatter plot with the actual total cases on the x-axis and the predicted total cases on the y-axis. Each point on the plot represents a test data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "O2q7OQNzq2he",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# evaluating the performance of the model using metrics such as mean absolute error (MAE) and root mean squared error (RMSE)\n",
    "\n",
    "y_pred = poisson_reg.predict(df_cleaned_test)\n",
    "print(\"MAE:\", mean_absolute_error(cases_test, y_pred))\n",
    "print(\"RMSE:\", mean_squared_error(cases_test, y_pred, squared=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "C7Jl9oPkCs-P",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Question 2.3: Performance of the Model\n",
    "\n",
    "Engage in discussion with your pod to share your observations from the additional changes above.\n",
    "- What did you observe?\n",
    "- Reflect on how different factors may affect the performance of the model.\n",
    "- Brainstorm as a group additional ways to improve the model's performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec3c125",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove explanation\n",
    "\"\"\"\n",
    "Possible discussion: In terms of a 1:1 truth versus prediction, both models still struggle to predict case numbers larger than 50. However R-squared values do increase, especially for the Poisson model, and there is a slight impact on the other error metrics. Note also, as mentioned, the Possion model does not predict negative case values.\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GjB_Kfmtkhd3",
   "metadata": {
    "execution": {}
   },
   "source": [
    "\n",
    "# Section 3: Decision Trees\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ftQG5z97vhix",
   "metadata": {
    "execution": {}
   },
   "source": [
    "In the field of climate science, decision trees and random forests can be powerful tools for making predictions and analyzing complex data.\n",
    "\n",
    "A decision tree is a type of model that is constructed by recursively splitting the data based on the values of the input features. Each internal node in the tree represents a decision based on the value of a feature, and each leaf node represents a prediction. Decision trees are easy to interpret and can capture complex, nonlinear relationships in the data.\n",
    "\n",
    "Random forests are a type of ensemble model that combines multiple decision trees to make predictions. Each tree in the forest is trained on a random subset of the data, and the final prediction is the average of the predictions from all the trees. Random forests are particularly useful in climate science because they can handle high-dimensional data with many features and can capture both linear and nonlinear relationships.\n",
    "\n",
    "By using decision trees and random forests, climate scientists can make accurate predictions about a variety of climate-related variables, such as temperature, precipitation, and sea-level rise. They can also gain insights into the complex relationships between different variables and identify important features that contribute to these relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6HDz0Wae8BUQ",
   "metadata": {
    "execution": {}
   },
   "source": [
    " By training a Random Forest Model in this tutorial, we can better understand the relationship between climate variables and dengue fever cases, and potentially improve our ability to predict and prevent outbreaks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UnpSMiiu8BUT",
   "metadata": {
    "execution": {}
   },
   "source": [
    "\n",
    "## Section 3.1: Fitting Model and Analyzing Results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0C_hBsqj8BUT",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Next, you will train a random forest regression model using scikit-learn's RandomForestRegressor class, with default hyperparameters. Use the documentation of the method [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) to help you if needed. Evaluate the model's performance on the training and test data and make a scatter plot of predicted vs actual cases for the test data. \n",
    "Use `RandomForestRegressor()` which we already imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "B37Yys8Y8BUU",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# train a random forest regression model\n",
    "\n",
    "# use the RandomForestRegressor we imported earlier\n",
    "rf = ...\n",
    "\n",
    "# run fit on 'df_cleaned_train' and 'cases_train'\n",
    "_ = ...\n",
    "\n",
    "# evaluate the model's performance on the training and testing data\n",
    "# calculate accuracy by calling rf.score() on 'df_cleaned_train' and 'cases_train'\n",
    "print(\"R^2 on training data is: \")\n",
    "print(...)\n",
    "\n",
    "print(\"R^2 on test data is: \")\n",
    "print(...)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "# plot the predicted vs. actual total cases on the test data\n",
    "_ = ...\n",
    "\n",
    "# add 1:1 line\n",
    "ax.plot(np.array(ax.get_xlim()), np.array(ax.get_xlim()), \"k-\")\n",
    "\n",
    "ax.set_xlabel(\"Actual Total Cases\")\n",
    "ax.set_ylabel(\"Predicted Total Cases\")\n",
    "ax.set_title(\"Random Forest Regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uX736VJs8BUU",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "\n",
    "# train a random forest regression model\n",
    "\n",
    "# use the RandomForestRegressor we imported earlier\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "# run fit on 'df_cleaned_train' and 'cases_train'\n",
    "_ = rf.fit(df_cleaned_train, cases_train)\n",
    "\n",
    "# evaluate the model's performance on the training and testing data\n",
    "# calculate accuracy by calling rf.score() on 'df_cleaned_train' and 'cases_train'\n",
    "print(\"R^2 on training data is: \")\n",
    "print(rf.score(df_cleaned_train, cases_train))\n",
    "\n",
    "print(\"R^2 on test data is: \")\n",
    "print(rf.score(df_cleaned_test, cases_test))\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "# plot the predicted vs. actual total cases on the test data\n",
    "_ = ax.scatter(cases_test, rf.predict(df_cleaned_test))\n",
    "\n",
    "# add 1:1 line\n",
    "ax.plot(np.array(ax.get_xlim()), np.array(ax.get_xlim()), \"k-\")\n",
    "\n",
    "ax.set_xlabel(\"Actual Total Cases\")\n",
    "ax.set_ylabel(\"Predicted Total Cases\")\n",
    "ax.set_title(\"Random Forest Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "N-GIWnvNPwDk",
   "metadata": {
    "execution": {}
   },
   "source": [
    "<details>\n",
    "<summary> <font color='orangered'>Click here description of plot  </font></summary>\n",
    "The plot generated is a scatter plot with the actual total cases on the x-axis and the predicted total cases on the y-axis. Each point on the plot represents a test data point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "StIShdNq8BUV",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# evaluating the performance of the model using metrics such as mean absolute error (MAE) and root mean squared error (MSE)\n",
    "\n",
    "y_pred = rf.predict(df_cleaned_test)\n",
    "print(\"MAE:\", mean_absolute_error(cases_test, y_pred))\n",
    "print(\"RMSE:\", mean_squared_error(cases_test, y_pred, squared=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xoZ3jPdk8BUV",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Question 3.1: Reflecting on the Performance \n",
    "Please think and discuss the following questions with your pod members:\n",
    "\n",
    "1. How does the performance of the random forest model compare to that of the linear regression model?\n",
    "2. How does the performance on the test data compare to the performance on the training data?\n",
    "3. What could be the reason behind performing well on the training data but poorly on the test data? Hint: Look up 'overfitting'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd54734e",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "#  to_remove explanation\n",
    "\"\"\"\n",
    "1. The random forest model generally performs better than the linear regression model as it can capture non-linear relationships between features and target.\n",
    "2. The low performance of the model on the test set might be due to the model learning the noise in the training data instead of the underlying patterns.\n",
    "3. Overfitting is the term for good training performance but poor test performance, where the model fits the training data too closely and fails to generalize to new data.\n",
    "4. Solutions to handle overfitting include reducing model complexity, increasing dataset size, using regularization, or cross-validation. Ensemble models such as random forests also do inherently help control overfitting by averaging many different models\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VX-uYtUS8BUV",
   "metadata": {
    "execution": {}
   },
   "source": [
    "\n",
    "## Section 3.2: Looking at Feature Importance\n",
    "\n",
    "When we train a model to predict an outcome, it's important to understand which inputs to the model are most important in driving that prediction. This is where 'feature importance' methods come in.\n",
    "\n",
    "One way to measure feature importance is by using the permutation method. This involves randomly shuffling the values of a feature and testing the performance of the model with these permuted values. The amount that the model performance decreases when the feature's values are permuted can provide an indication of how important it is.\n",
    "\n",
    "For climate scientists, understanding feature importance can help identify key variables that contribute to predicting important outcomes, such as temperature or precipitation patterns.\n",
    "\n",
    "Thankfully, Sci-kit learn has a method that implements permutation importance, and outputs a normalized measure of how much each feature impacts performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afXyxUfgdM67",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# execute this cell to enable the plotting function to be used for plotting performance of our model in next cell: `plot_feature_importance`\n",
    "\n",
    "\n",
    "def plot_feature_importance(perm_feat_imp):\n",
    "    # increase the size of the plot for better readability\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    # plot the feature importance with error bars in navy blue color\n",
    "    ax.errorbar(\n",
    "        np.arange(len(df_cleaned.columns)),\n",
    "        perm_feat_imp[\"importances_mean\"],\n",
    "        perm_feat_imp[\"importances_std\"],\n",
    "        fmt=\"o\",\n",
    "        capsize=5,\n",
    "        markersize=5,\n",
    "        color=\"navy\",\n",
    "    )\n",
    "\n",
    "    # set the x-axis and y-axis labels and title\n",
    "    ax.set_xlabel(\"Features\", fontsize=14)\n",
    "    ax.set_ylabel(\"Importance\", fontsize=14)\n",
    "    ax.set_title(\"Feature Importance Plot\", fontsize=16)\n",
    "\n",
    "    # rotate the x-axis labels for better readability and add labels\n",
    "    feature_names = df_cleaned.columns\n",
    "    x_ticks = np.arange(len(feature_names))\n",
    "    ax.set_xticks(x_ticks)\n",
    "    ax.set_xticklabels(feature_names, rotation=45, ha=\"right\", fontsize=12)\n",
    "\n",
    "    # add gridlines for better visualization\n",
    "    ax.grid(True, axis=\"y\", linestyle=\"--\")\n",
    "\n",
    "    # bar plot\n",
    "    ax.bar(\n",
    "        np.arange(len(df_cleaned.columns)),\n",
    "        perm_feat_imp[\"importances_mean\"],\n",
    "        color=\"navy\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LuqdCv4FdM68",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Plot the feature importance of each input to the model\n",
    "\n",
    "# Use permutation_importance to calculate the feature importances of the trained random forest model\n",
    "# df_cleaned_train contains the preprocessed training dataset, cases_train contains the actual number of dengue fever cases in the training dataset\n",
    "# n_repeats specifies how many times the feature importances are calculated for each feature\n",
    "# random_state is used to seed the random number generator for reproducibility\n",
    "perm_feat_imp = permutation_importance(\n",
    "    rf, df_cleaned_train, cases_train, n_repeats=10, random_state=0\n",
    ")\n",
    "\n",
    "# Create a plot of the feature importances\n",
    "plot_feature_importance(perm_feat_imp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VTbaHX2S8BUW",
   "metadata": {
    "execution": {}
   },
   "source": [
    "<details>\n",
    "<summary> <font color='orangered'>Click here for description of plot </font></summary>\n",
    "\n",
    "The plot generated is a feature importance plot that provides insights into the importance of different features in predicting the target variable. Let's break down the key elements and address the specific issues raised.\n",
    "\n",
    "1. Feature Importance Representation:\n",
    "- Each bar in the plot represents a feature from the dataset.\n",
    "- The height of each bar represents the relative feature importance value.\n",
    "- Specifically, importance is measured as the decrease in performance that comes from permuting that feature.\n",
    "- Negative values typically indicate that the model performed better if the feature was removed.\n",
    "\n",
    "2. Error Bars and Variability:\n",
    "- The error bars present around each feature's bar represent the variability in importance values.\n",
    "- They indicate the uncertainty or variability in the importance estimates calculated through repeated permutations.\n",
    "- A longer error bar suggests higher variability, meaning the importance value for that feature may change significantly with different permutations.\n",
    "- Conversely, a shorter error bar indicates lower variability, indicating that the importance value is relatively stable and less influenced by random permutations.\n",
    "\n",
    "Understanding the feature importance plot empowers us to identify the most influential factors within our dataset. By recognizing these crucial features, we gain deeper insights into the underlying relationships within climate science data. This knowledge is invaluable for further analysis and informed decision-making processes within our field.\n",
    "\n",
    "It's important to note that the interpretation of feature importance can vary depending on the dataset and modeling technique employed. To gain a comprehensive understanding of feature importance analysis in the context of climate science, it is advisable to consult domain experts and explore additional resources tailored to your specific interests.\n",
    "\n",
    "By delving into the world of feature importance, we unlock the potential to unravel the intricate dynamics of our climate science data and make meaningful contributions to this fascinating field of study.\n",
    "\n",
    " </summary>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EJ1y4x4X8BUW",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Question 3.2: Climate Connection\n",
    "Please think and discuss the following questions with your pod members:\n",
    "\n",
    "1. Which features were most important? Why do you suspect they would be?\n",
    "2. Which features were not? Are there other variables not included here you think that would be more relevant than those that were deemed 'not important'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb20e272",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove explanation\n",
    "\"\"\"\n",
    "1. The plot above shows that the min_air_temp (minimum air temperature) has the highest importance score, followed by week of year and tdtr (diurnal temperature range). Warmer environments are more conducive to mosquito populations, and the week of year may be important due to the season.\n",
    "2. Surprisingly, these results suggest that some of the precipitation variables were not very helpful. One variable that might be more relevant to mosquito populatoins is an esimate of standing water.\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IFZVBmzu8BUX",
   "metadata": {
    "execution": {}
   },
   "source": [
    "\n",
    "## (Bonus) Section 3.3: Comparing Feature Importance Methods\n",
    "\n",
    "The Random Forest Regression model also has a built-in estimation of feature importance. This estimation comes directly from how the decision trees are trained; specifically, it is a measure of how useful the feature is at splitting the data averaged across all nodes and trees. We can access these values directly from the trained model.\n",
    "\n",
    "Different methods of estimating feature importance can come to different conclusions and may have different biases. Therefore, it is good to compare estimations across methods. How do these two methods compare? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i2-RQwIw8BUX",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# set the figure size for better readability\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# create a bar chart of the feature importances returned by the random forest model\n",
    "ax.bar(np.arange(len(rf.feature_importances_)), rf.feature_importances_, color=\"navy\")\n",
    "\n",
    "# set the x-axis and y-axis labels and title\n",
    "ax.set_xlabel(\"Features\", fontsize=14)\n",
    "ax.set_ylabel(\"Importance\", fontsize=14)\n",
    "ax.set_title(\"Feature Importance Plot\", fontsize=16)\n",
    "\n",
    "# rotate the x-axis labels for better readability and add labels\n",
    "feature_names = df_cleaned.columns\n",
    "x_ticks = np.arange(len(feature_names))\n",
    "ax.set_xticks(x_ticks)\n",
    "ax.set_xticklabels(feature_names, rotation=45, ha=\"right\", fontsize=12)\n",
    "\n",
    "# set the y-axis limit to better visualize the differences in feature importance\n",
    "ax.set_ylim(0, rf.feature_importances_.max() * 1.1)\n",
    "\n",
    "# add gridlines for better visualization\n",
    "ax.grid(True, axis=\"y\", linestyle=\"--\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cXiY9CWU8BUX",
   "metadata": {
    "execution": {}
   },
   "source": [
    "\n",
    "<details>\n",
    "<summary> <font color='orangered'>Click here for description of plot </font></summary>\n",
    "\n",
    "The bar chart displays the feature importances returned by the random forest model. Each bar represents the relative importance of each feature in predicting the number of dengue fever cases in the preprocessed dataset. The y-axis represents the importance of the features, while the x-axis displays the name of the features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d9b28b",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Question 3.3: Climate Connection\n",
    "Please think and discuss the following questions with your pod members:\n",
    "\n",
    "1. How does this compare to the feature importance figure calculated using the permuation method above?\n",
    "2. Do the differences agree more or less with your own expectations of which variables would be most important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c24e8f",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove explanation\n",
    "\"\"\"\n",
    "1. Unlike the previous estimate of importance, here week of year, temperature variables and precipitation and humidity are all important. This underpins the importance of using multiple methods for estimating feature importance.\n",
    "2. The important features with the built-in method shown here agree likely more with expectations: hot and humid environments are both supportive of mosquito populations. Note: we have used the training data to evaluate feature importance (this is mandatory for the built-in method); different results may come from using the test data.\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Rod5BrQakSJf",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Summary\n",
    "\n",
    "In this tutorial, you explored various methods of analyzing the Dengue Fever dataset from a climate science perspective. You started by using pandas to handle the data and visualizing trends and anomalies. Next, you applied linear regression to model the data and handle categorical and integer-valued data. Finally, you applied random forest regression to improve the performance of the model and learned about feature importance. Overall, learners gained practical experience in using different modeling techniques to analyze and make predictions about real-world climate data. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ytSWAGmQ8BUY",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Extra Exercises or Project Ideas\n",
    "\n",
    "1. Try experimenting with different hyperparameters for the random forest model, such as n_estimators, max_depth, and min_samples_leaf. How do these hyperparameters affect the performance of the model? \n",
    "\n",
    "2. Try using a different machine learning algorithm to predict the number of Dengue fever cases, such as a support vector machine. How does the performance of these algorithms compare to the random forest model?\n",
    "\n",
    "3. Try using a different dataset to predict the number of cases of a different disease or health condition. How does the preprocessing and modeling process differ for this dataset compared to the Dengue fever dataset?\n",
    "\n",
    "4. Try visualizing the decision tree of the random forest model using the plot_tree function of the sklearn package. What insights can you gain from the visualization of the decision tree?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7bd6e6",
   "metadata": {
    "execution": {}
   },
   "source": [
    "#*Resources\n",
    "\n",
    "Data from this tutorial can be accessed [here](https://www.drivendata.org/competitions/44/dengai-predicting-disease-spread/page/81/)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "W2D5_Tutorial2",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
