{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8311c9e7",
   "metadata": {
    "execution": {}
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ClimateMatchAcademy/course-content/blob/main/tutorials/W2D5_ClimateResponse-AdaptationImpact/W2D5_Tutorial3.ipynb) &nbsp; <a href=\"https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/ClimateMatchAcademy/course-content/main/tutorials/W2D5_ClimateResponse-AdaptationImpact/W2D5_Tutorial3.ipynb\" target=\"_parent\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open in Kaggle\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "myt07YFyNgmw",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Tutorial 3:  Logistic Regression and ANN on the Remote Sensing Crop Dataset\n",
    "\n",
    "**Week 2, Day 5: Adaptation and Impact**\n",
    "\n",
    "__Content creators:__ Deepak Mewada, Grace Lindsay\n",
    "\n",
    "__Content reviewers:__ Dionessa Biton, Younkap Nina Duplex, Sloane Garelick, Zahra Khodakaramimaghsoud, Peter Ohue, Jenna Pearson, Michael Rizzo, Derick Temfack, Peizhen Yang, Cheng Zhang, Chi Zhang, Ohad Zivan\n",
    "\n",
    "__Content editors:__ Jenna Pearson, Chi Zhang, Ohad Zivan\n",
    "\n",
    "__Production editors:__ Wesley Banfield, Jenna Pearson, Chi Zhang, Ohad Zivan\n",
    "\n",
    "**Our 2023 Sponsors:** NASA TOPS and Google DeepMind"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kDQc1jnoNWcp",
   "metadata": {
    "execution": {}
   },
   "source": [
    "\n",
    "# Tutorial Objectives\n",
    "\n",
    "Welcome to tutorial 3 of a series of tutorials focusing on understanding the role of data science and machine learning in addressing the impact of climate change and adapting to it.\n",
    "\n",
    "This tutorial is designed to provide an in-depth understanding of logistic regression and artificial neural networks on the Remote Sensing Crop Dataset. Upon completion of the tutorial, you will be able to:\n",
    "\n",
    "- Load and visualize the Remote Sensing Crop Dataset\n",
    "- Fit a logistic regression model on the dataset and evaluate its performance using various metrics\n",
    "- Apply more feature importance methods such as missing features-only and permutation importance\n",
    "- Implement an artificial neural network on the crop dataset and learn about overfitting and methods to prevent it\n",
    "- Reflect on the performance of the models and the importance measures and discuss their implications\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Vtq0OyoRNPcc",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Kwsl6-KNNPcc",
   "metadata": {
    "execution": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np  # NumPy for numerical computing\n",
    "import pandas as pd  # Pandas for data manipulation\n",
    "import matplotlib.pyplot as plt  # Matplotlib for visualization\n",
    "import os\n",
    "import pooch\n",
    "import tempfile\n",
    "import seaborn as sns\n",
    "import xarray as xr\n",
    "\n",
    "# import the LogisticRegression class from the sklearn.linear_model module\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.metrics as skm\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.neural_network import MLPClassifier  # A type of artificial neural network\n",
    "from sklearn.exceptions import (\n",
    "    ConvergenceWarning,\n",
    ")  # A warning message that may appear during model training\n",
    "import warnings  # A module for managing warning messages\n",
    "\n",
    "# ignore the convergence warnings that may appear during model training\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf7d640",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Figure Settings\n",
    "import ipywidgets as widgets  # interactive display\n",
    "\n",
    "plt.style.use(\n",
    "    \"https://raw.githubusercontent.com/ClimateMatchAcademy/course-content/main/cma.mplstyle\"\n",
    ")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2491ffb-8c95-42f1-83cd-d4a2d5a631df",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Helper functions\n",
    "\n",
    "\n",
    "def pooch_load(filelocation=None, filename=None, processor=None):\n",
    "    shared_location = \"/home/jovyan/shared/Data/tutorials/W2D5_ClimateResponse-AdaptationImpact\"  # this is different for each day\n",
    "    user_temp_cache = tempfile.gettempdir()\n",
    "\n",
    "    if os.path.exists(os.path.join(shared_location, filename)):\n",
    "        file = os.path.join(shared_location, filename)\n",
    "    else:\n",
    "        file = pooch.retrieve(\n",
    "            filelocation,\n",
    "            known_hash=None,\n",
    "            fname=os.path.join(user_temp_cache, filename),\n",
    "            processor=processor,\n",
    "        )\n",
    "\n",
    "    return file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vMap3TCmNPcc",
   "metadata": {
    "execution": {}
   },
   "source": [
    "<details>\n",
    "<summary> <font color='Red'>Click here if you are running on local machine or you encounter any error while importing   </font></summary>\n",
    "NOTE: Please note that if you are running this code on a local machine and encounter an error while importing a library, make sure to install the library via pip. For example, if you receive a `\"ModuleNotFoundError: No module named 'google.colab'\" `error while using 'Colab', please run `\"pip install google\"` to install the required module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rEC6O7Twv_ER",
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @title Plotting functions\n",
    "def plot_feature_performance(\n",
    "    missing_feature_performance, only_feature_performance, feature_names\n",
    "):\n",
    "    # Create figure with subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # Plot missing feature performance\n",
    "    ax1.bar(range(12), missing_feature_performance)\n",
    "    ax1.set_xticks(range(12))\n",
    "    ax1.set_xticklabels(feature_names, rotation=90)\n",
    "    ax1.set_ylabel(\"Performance\")\n",
    "    ax1.set_title(\"Impact of Missing Features on Model Performance\")\n",
    "    ax1.yaxis.grid(True)\n",
    "\n",
    "    # Plot only one feature performance\n",
    "    ax2.bar(range(12), only_feature_performance)\n",
    "    ax2.set_xticks(range(12))\n",
    "    ax2.set_xticklabels(feature_names, rotation=90)\n",
    "    ax2.set_ylabel(\"Performance\")\n",
    "    ax2.set_title(\"Impact of Single Features on Model Performance\")\n",
    "    ax2.yaxis.grid(True)\n",
    "\n",
    "    # Adjust spacing between subplots\n",
    "    plt.subplots_adjust(wspace=0.4)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_permutation_feature_importance(perm_feat_imp, X_test, feature_names):\n",
    "    \"\"\"\n",
    "    Plots feature importance using the permutation importance method.\n",
    "\n",
    "    Args:\n",
    "        perm_feat_imp (dict): A dictionary containing feature importance scores and statistics.\n",
    "        X_test (ndarray): The testing data.\n",
    "    \"\"\"\n",
    "    # create a figure and axis object using subplots\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # create a bar plot for feature importance\n",
    "    # set the x-axis to be the feature index\n",
    "    # set the y-axis to be the mean importance score\n",
    "    # set the error bar to be the standard deviation of importance scores\n",
    "    ax.bar(\n",
    "        np.arange(X_test.shape[1]),\n",
    "        perm_feat_imp[\"importances_mean\"],\n",
    "        yerr=perm_feat_imp[\"importances_std\"],\n",
    "    )\n",
    "\n",
    "    # set the x-tick labels to be the feature names\n",
    "    ax.set_xticks(np.arange(X_test.shape[1]))\n",
    "    ax.set_xticklabels(feature_names, rotation=90)\n",
    "\n",
    "    # set the x and y axis labels and title\n",
    "    ax.set_xlabel(\"Feature index\")\n",
    "    ax.set_ylabel(\"feature Importance\")\n",
    "    ax.set_title(\"Feature Importance using Permutation Importance Method\")\n",
    "\n",
    "    # display the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa45ac74-e630-4567-892f-6a7018d10140",
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @title Video 1: Crops Dataset\n",
    "\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import YouTubeVideo\n",
    "from IPython.display import IFrame\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "class PlayVideo(IFrame):\n",
    "  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n",
    "    self.id = id\n",
    "    if source == 'Bilibili':\n",
    "      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n",
    "    elif source == 'Osf':\n",
    "      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n",
    "    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "\n",
    "def display_videos(video_ids, W=400, H=300, fs=1):\n",
    "  tab_contents = []\n",
    "  for i, video_id in enumerate(video_ids):\n",
    "    out = widgets.Output()\n",
    "    with out:\n",
    "      if video_ids[i][0] == 'Youtube':\n",
    "        video = YouTubeVideo(id=video_ids[i][1], width=W,\n",
    "                             height=H, fs=fs, rel=0)\n",
    "        print(f'Video available at https://youtube.com/watch?v={video.id}')\n",
    "      else:\n",
    "        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n",
    "                          height=H, fs=fs, autoplay=False)\n",
    "        if video_ids[i][0] == 'Bilibili':\n",
    "          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n",
    "        elif video_ids[i][0] == 'Osf':\n",
    "          print(f'Video available at https://osf.io/{video.id}')\n",
    "      display(video)\n",
    "    tab_contents.append(out)\n",
    "  return tab_contents\n",
    "\n",
    "\n",
    "video_ids = [('Youtube', 'Cve_NH3w96g'), ('Bilibili', 'BV1GW4y1Z7tZ')]\n",
    "tab_contents = display_videos(video_ids, W=730, H=410)\n",
    "tabs = widgets.Tab()\n",
    "tabs.children = tab_contents\n",
    "for i in range(len(tab_contents)):\n",
    "  tabs.set_title(i, video_ids[i][0])\n",
    "display(tabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ec652e-bc09-4df4-b10c-d038ebc4a241",
   "metadata": {
    "cellView": "form",
    "execution": {},
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# @markdown\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import IFrame\n",
    "\n",
    "link_id = \"5s8gq\"\n",
    "\n",
    "download_link = f\"https://osf.io/download/{link_id}/\"\n",
    "render_link = f\"https://mfr.ca-1.osf.io/render?url=https://osf.io/{link_id}/?direct%26mode=render%26action=download%26mode=render\"\n",
    "# @markdown\n",
    "out = widgets.Output()\n",
    "with out:\n",
    "    print(f\"If you want to download the slides: {download_link}\")\n",
    "    display(IFrame(src=f\"{render_link}\", width=730, height=410))\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nULavCfq4o07",
   "metadata": {
    "execution": {}
   },
   "source": [
    "\n",
    "# Section 1: Remote Sensing Crops Dataset Exploration\n",
    "\n",
    "In this section we will load, visualize and explore the remote sensing data collected from the Sentinel-2 satellite to find crops in the Togolese Republic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j9n17gXFOIXX",
   "metadata": {
    "execution": {}
   },
   "source": [
    "\n",
    "## Section 1.1:  Loading  the Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5c8b85",
   "metadata": {
    "execution": {}
   },
   "source": [
    "As discussed in the video, we will use remote sensing data collected from the Sentinel-2 satellite to find crops in the Togolese Republic. The data is described in [this paper](https://arxiv.org/pdf/2006.16866.pdf), and was accessed through [this repo](https://github.com/nasaharvest/cropharvest)). \n",
    "\n",
    "The data contains 1290 data points in the training set and 306 in the test set. Each data point represents one spatial location. Note: the data was collected for 12 months but we are only using the first month's values. As discussed in the video, the data contains normalized values from all Sentintel-2 bands, except B1 and B10, plus the pre-calculated NDVI. Therefore, each data point has 12 features in total. Each point is labeled as containing crops (1) or not (0).   \n",
    "\n",
    "If you want to know more about what these bands (feature) mean, you can refer to [this resource](https://en.wikipedia.org/wiki/Sentinel-2#cite_note-15:~:text=Spectral%20bands%20for%20the%20Sentinel%2D2%20sensors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170149fb",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# load training data\n",
    "filename_togo_crops_training = \"training.nc\"\n",
    "url_togo_crops_training = \"https://osf.io/7m8cz/download\"\n",
    "\n",
    "training_ds = xr.open_dataset(\n",
    "    pooch_load(url_togo_crops_training, filename_togo_crops_training)\n",
    ")\n",
    "training_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81eabf7c",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# load test data\n",
    "filename_togo_crops_test = \"test.nc\"\n",
    "url_togo_crops_test = \"https://osf.io/7r6cp/download\"\n",
    "\n",
    "test_ds = xr.open_dataset(pooch_load(url_togo_crops_test, filename_togo_crops_test))\n",
    "test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02db7bb2",
   "metadata": {
    "execution": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = training_ds[\"X_train\"][:]\n",
    "y_train = training_ds[\"y_train\"][:]\n",
    "X_test = test_ds[\"X_test\"][:]\n",
    "y_test = test_ds[\"y_test\"][:]\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254505a9",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# see the features/bands used:\n",
    "feature_names = training_ds[\"bands\"].values\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XtgjDdu_eeC-",
   "metadata": {
    "execution": {}
   },
   "source": [
    "\n",
    "In the next few sections we will be learning about various ways to visualise the data to find some trends.\n",
    "\n",
    "## Section 1.2: Visualizing the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b6959e",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Coding Exercises 1.2\n",
    "\n",
    "Plot histograms of the training values of each feature. Specifically, for each feature, make a single plot that contains two histograms: one of the values for locations with crops and one for those without. Set the bins the same for each and reduce the transparency of each so that both are visible and comparable. Also print the percentage of data points that have crops in them. It is important to understand how balanced a data set is when analyzing performance.  \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mZgnpC9Y_4kk",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# separate the positive and negative classes\n",
    "pos_inds = np.where(\n",
    "    y_train == ...\n",
    ")  # indices of positive class (i.e positive class will have y_train as 1)\n",
    "neg_inds = np.where(\n",
    "    y_train == ...\n",
    ")  # indices of negative class (i.e positive class will have y_train as 0)\n",
    "\n",
    "# create subplots for each feature\n",
    "fig, ax = plt.subplots(3, 4)\n",
    "ax = ax.flatten()\n",
    "for i in range(12):\n",
    "    # plot histograms of the positive and negative classes for the current feature\n",
    "    n, bins, patches = ax[i].hist(\n",
    "        X_train[:, i][...], alpha=0.5\n",
    "    )  # histogram of the positive class\n",
    "    ax[i].hist(\n",
    "        X_train[:, i][...], bins=bins, alpha=0.5\n",
    "    )  # histogram of the negative class\n",
    "    ax[i].set_title(\n",
    "        feature_names[i]\n",
    "    )  # set the title of the subplot to the current feature number\n",
    "\n",
    "fig.tight_layout()  # adjust spacing between subplots\n",
    "\n",
    "# calculate % by (number of positive/(number of positive + number of negative)); can use len to\n",
    "print(\"Percentage of positive samples: \" + str(...))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889b41b7",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "\n",
    "# separate the positive and negative classes\n",
    "pos_inds = np.where(\n",
    "    y_train == 1\n",
    ")  # indices of positive class (i.e positive class will have y_train as 1)\n",
    "neg_inds = np.where(\n",
    "    y_train == 0\n",
    ")  # indices of negative class (i.e positive class will have y_train as 0)\n",
    "\n",
    "# create subplots for each feature\n",
    "fig, ax = plt.subplots(3, 4)\n",
    "ax = ax.flatten()\n",
    "for i in range(12):\n",
    "    # plot histograms of the positive and negative classes for the current feature\n",
    "    n, bins, patches = ax[i].hist(\n",
    "        X_train[:, i][pos_inds], alpha=0.5\n",
    "    )  # histogram of the positive class\n",
    "    ax[i].hist(\n",
    "        X_train[:, i][neg_inds], bins=bins, alpha=0.5\n",
    "    )  # histogram of the negative class\n",
    "    ax[i].set_title(\n",
    "        feature_names[i]\n",
    "    )  # set the title of the subplot to the current feature number\n",
    "\n",
    "fig.tight_layout()  # adjust spacing between subplots\n",
    "\n",
    "# calculate % by (number of positive/(number of positive + number of negative)); can use len to\n",
    "print(\"Percentage of positive samples: \" + str(len(pos_inds) / (len(pos_inds) + len(neg_inds))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kbSM9SikU_ge",
   "metadata": {
    "execution": {}
   },
   "source": [
    "<details>\n",
    "<summary> <font color='orangered'>Click here description of histogram  </font></summary>\n",
    "\n",
    "The plot generated shows the histograms of the positive and negative classes for each feature of the dataset. Each subplot represents a single feature and shows the distribution of values for that feature across the positive and negative classes. The histograms of the positive and negative classes are overlaid on each other with different colors for better comparison. The title of each subplot indicates the corresponding feature number.\n",
    "\n",
    "By examining the histograms, we can gain insights into the relationship between each feature and the target variable (positive or negative class). For example, we can observe whether a particular feature has a similar distribution across both classes or if there is a significant difference in the distribution for the positive and negative classes. This can help us decide which features are more relevant for predicting the target variable and may be useful for feature selection or engineering. Additionally, the percentage of positive samples is printed, which gives us an idea of the class balance in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l07SCayNAB8H",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Questions 1.2\n",
    "\n",
    "1. Based on these plots, do you think the B11 would be useful for identifying crops? What about B6?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de32400",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove explanation\n",
    "\"\"\"\n",
    "1. B11 could be helpful for identifying crops as the distribution of its values is different when crops are present. This is not really the case for B6.\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yZC3Qv3zu2T9",
   "metadata": {
    "execution": {}
   },
   "source": [
    "\n",
    "## (Bonus) Section 1.3: Visualising Correlation Between the Features\n",
    "\n",
    "Looking at how different features of the data relate to each other can help us better understand the data and can be important when thinking about model building."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a_pLI1SjhB2W",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Coding Exercise 1.3.1\n",
    "\n",
    "First we produce a heatmap showing the correlation coefficients between the features, with blue indicating negative correlation and red indicating positive correlation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fb4a69",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# calculate the correlation matrix\n",
    "corr = np.corrcoef(X_train, rowvar=False)\n",
    "\n",
    "# plot the correlation matrix using a heatmap\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "sns.heatmap(\n",
    "    corr,\n",
    "    cmap=\"coolwarm\",\n",
    "    center=0,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    xticklabels=feature_names,\n",
    "    yticklabels=feature_names,\n",
    "    ax=ax,\n",
    ")\n",
    "ax.set_title(\"Correlation between features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rkHPvqdDmf66",
   "metadata": {
    "execution": {}
   },
   "source": [
    "<details>\n",
    "<summary> <font color='orangered'>But what is Correlation coefficient? </font></summary>\n",
    "\n",
    "Correlation coefficients represent the strength and direction of the relationship between two variables. They range from -1 to +1, with +1 indicating a strong positive correlation, -1 indicating a strong negative correlation, and 0 indicating no correlation. In a heatmap, colors represent the correlation values, with darker colors indicating stronger correlations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cs37d_dGub2l",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The correlation coefficient summarizes the relationship between two features. We can see this relationship more directly by plotting scatter plots of all the training data points. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ToQiMNJ2hPEW",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Coding Exercise 1.3.2\n",
    "In this example, we create a 12x12 grid of subplots, with each subplot showing a scatter plot of two features. The color of the dots in the scatter plot represents the label of the data point (y_train). We set the cmap parameter of `plt.scatter` to 'bwr' to use a colormap that goes from blue for negative labels to red for positive labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Rl3-ue1unpQM",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# plot the data using scatter plots\n",
    "fig, axs = plt.subplots(12, 12, figsize=(20, 20))\n",
    "for i in range(12):\n",
    "    for j in range(12):\n",
    "        axs[i, j].scatter(X_train[:, i], X_train[:, j], c=y_train, cmap=\"bwr\")\n",
    "        axs[i, j].set_xlabel(feature_names[j])\n",
    "        axs[i, j].set_ylabel(feature_names[i])\n",
    "        axs[i, j].set_xticks([])\n",
    "        axs[i, j].set_yticks([])\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e97d810",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Questions 1.3.2\n",
    "\n",
    "1. Based on what you know about remote sensing and hyperspectral data, does it make sense that some pairs of features may be more correlated than others?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20492c67-b98b-4de5-9c75-e563c3fb8c39",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove explanation\n",
    "\"\"\"\n",
    "1. Hyperspectral data consists of many bands (sometimes hundreds) that capture reflectance values across a wide range of the electromagnetic spectrum. Each band corresponds to a specific range of wavelengths. It is possible that bands that sample nearby wavelengths will show correlated results.\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t4ij2getlJBo",
   "metadata": {
    "execution": {}
   },
   "source": [
    "\n",
    "# Section 2: Logistic Regression on Crops Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g6i3jRNRXgBY",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now that we understand our remote sensing data set we can train a model to classify each point as either containing or not containing crops. The data is already separated into training and test sets. Use what you've learned to train a logistic regression model on the training data. Evaluate the model separately on both the training set and test set according to the overall classification accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0REr4qfCYBvg",
   "metadata": {
    "execution": {}
   },
   "source": [
    "\n",
    "## Section 2.1: Model Fitting on Data\n",
    "\n",
    "In the following section, we will be using the data loaded in the previous section. Specifically, we have the training data in the variables `X_train` and `y_train`, and the testing data in the variables `X_test` and `y_test`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wEn5ZO0CYqqY",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Coding Exercise 2.1: Fitting a Logistic Regression Model and Evaluation Using Scikit-Learn\n",
    "In this exercise, you will use `LogisticRegression` from [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) to train a logistic regression model on the Togo crops dataset. First, fit the model to the training data, and then evaluate its accuracy on both the training and test data using the `.score()` method. Finally, print the training and test accuracy scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58q3EL2lgJiA",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def evaluate_model(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Fits a logistic regression model to the training data and evaluates its accuracy on the training and test sets.\n",
    "\n",
    "    Parameters:\n",
    "    X_train (numpy.ndarray): The feature data for the training set.\n",
    "    y_train (numpy.ndarray): The target data for the training set.\n",
    "    X_test (numpy.ndarray): The feature data for the test set.\n",
    "    y_test (numpy.ndarray): The target data for the test set.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing the training accuracy and test accuracy of the trained model.\n",
    "    \"\"\"\n",
    "\n",
    "    # create an instance of the LogisticRegression class and fit it to the training data\n",
    "    trained_model = ...\n",
    "\n",
    "    # calculate the training and test accuracy of the trained model\n",
    "    train_accuracy = ...\n",
    "    test_accuracy = ...\n",
    "\n",
    "    # return the trained_model, training and test accuracy of the trained model\n",
    "    return trained_model, train_accuracy, test_accuracy\n",
    "\n",
    "\n",
    "trained_model, train_acc, test_acc = evaluate_model(X_train, y_train, X_test, y_test)\n",
    "print(\"Training Accuracy: \", train_acc)\n",
    "print(\"Test Accuracy: \", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oHRGwEJSgSKY",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "\n",
    "def evaluate_model(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Fits a logistic regression model to the training data and evaluates its accuracy on the training and test sets.\n",
    "\n",
    "    Parameters:\n",
    "    X_train (numpy.ndarray): The feature data for the training set.\n",
    "    y_train (numpy.ndarray): The target data for the training set.\n",
    "    X_test (numpy.ndarray): The feature data for the test set.\n",
    "    y_test (numpy.ndarray): The target data for the test set.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing the training accuracy and test accuracy of the trained model.\n",
    "    \"\"\"\n",
    "    # create an instance of the LogisticRegression class and fit it to the training data\n",
    "    trained_model = LogisticRegression().fit(X_train, y_train)\n",
    "\n",
    "    # calculate the training and test accuracy of the trained model\n",
    "    train_accuracy = trained_model.score(X_train, y_train)\n",
    "    test_accuracy = trained_model.score(X_test, y_test)\n",
    "\n",
    "    # return the trained_model, training and test accuracy of the trained model\n",
    "    return trained_model, train_accuracy, test_accuracy\n",
    "\n",
    "\n",
    "trained_model, train_acc, test_acc = evaluate_model(X_train, y_train, X_test, y_test)\n",
    "print(\"Training Accuracy: \", train_acc)\n",
    "print(\"Test Accuracy: \", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Vg4e7B_b7pk9",
   "metadata": {
    "execution": {}
   },
   "source": [
    "\n",
    "## Section 2.2:  Further Evaluation of Performance\n",
    "\n",
    "As discussed in the video, in some cases, overall accuracy of a machine learning model can be misleading, as it may not reveal how the model is performing in different areas. **Precision** and **recall** are two important metrics that can help evaluate the performance of a model in terms of its ability to correctly identify positive cases (precision) and to identify all positive cases (recall).\n",
    "\n",
    "In this section, we will calculate precision and recall on the test set of our model using scikit-learn's built-in evaluation functions: `precision_score()` and `recall_score()`. For more information on these functions, please refer to their documentation : [precision](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html) and [recall](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IkKku59geZxp",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Coding Exercise 2.2\n",
    "In this exercise, you will evaluate the performance of a machine learning model using precision and recall metrics. We will provide you with a trained model, a test set X_test and its corresponding labels y_test. Your task is to use scikit-learn's built-in evaluation functions, precision_score() and recall_score(), to calculate the precision and recall on the test set. Then, you will print the results.\n",
    "\n",
    "To complete this exercise, you will need to:\n",
    " \n",
    "- Use the trained model to make predictions on the test set.  \n",
    "- Calculate the recall score and precision score on the test set using scikit-learn's recall_score() and precision_score() functions, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Igns5rCSqwLy",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def evaluate_model_performance(trained_model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluate the performance of a trained model on a given test set suing recall and precision\n",
    "\n",
    "    Parameters:\n",
    "    trained_model (sklearn estimator): A trained scikit-learn estimator.\n",
    "    X_test (array-like): Test input data.\n",
    "    y_test (array-like): True labels for the test data.\n",
    "\n",
    "    Returns:\n",
    "    test_recall (float): The recall score on the test set.\n",
    "    test_precision (float): The precision score on the test set.\n",
    "    \"\"\"\n",
    "\n",
    "    # use the trained model to make predictions on the test set using predict()\n",
    "    pred_test = ...\n",
    "\n",
    "    # calculate the recall and precision scores on the test set using recall_score() and precision_score()\n",
    "    test_recall = ...\n",
    "    test_precision = ...\n",
    "\n",
    "    # return the recall and precision scores\n",
    "    return ..., ...\n",
    "\n",
    "\n",
    "# evaluate the performance of the trained model on the test set\n",
    "test_recall, test_precision = evaluate_model_performance(trained_model, X_test, y_test)\n",
    "print(\"Test Recall: \", test_recall)\n",
    "print(\"Test Precision: \", test_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63-Mwmzp-rB",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "\n",
    "def evaluate_model_performance(trained_model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluate the performance of a trained model on a given test set.\n",
    "\n",
    "    Parameters:\n",
    "    trained_model (sklearn estimator): A trained scikit-learn estimator.\n",
    "    X_test (array-like): Test input data.\n",
    "    y_test (array-like): True labels for the test data.\n",
    "\n",
    "    Returns:\n",
    "    test_recall (float): The recall score on the test set.\n",
    "    test_precision (float): The precision score on the test set.\n",
    "    \"\"\"\n",
    "    # use the trained model to make predictions on the test set using predict()\n",
    "    pred_test = trained_model.predict(X_test)\n",
    "\n",
    "    # calculate the recall and precision scores on the test set using recall_score() and precision_score()\n",
    "    test_recall = skm.recall_score(y_test, pred_test)\n",
    "    test_precision = skm.precision_score(y_test, pred_test)\n",
    "\n",
    "    # return the recall and precision scores\n",
    "    return test_recall, test_precision\n",
    "\n",
    "\n",
    "# evaluate the performance of the trained model on the test set\n",
    "test_recall, test_precision = evaluate_model_performance(trained_model, X_test, y_test)\n",
    "print(\"Test Recall: \", test_recall)\n",
    "print(\"Test Precision: \", test_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sMnxU6R7omcW",
   "metadata": {
    "execution": {}
   },
   "source": [
    "<details>\n",
    "<summary> <font color='orange'>But what is Precision and Recall? 🙋‍♂️  </font>\n",
    "\n",
    "</summary>Precision and recall are two commonly used metrics in binary classification tasks.\n",
    "\n",
    "Precision measures the accuracy of positive predictions made by a model. It is the ratio of true positive predictions to the total number of positive predictions (true positives + false positives). A high precision indicates a low false positive rate, meaning that when the model predicts a positive outcome, it is likely to be correct.\n",
    "\n",
    "Recall, also known as sensitivity or true positive rate, measures the ability of a model to identify positive instances correctly. It is the ratio of true positive predictions to the total number of actual positive instances (true positives + false negatives). A high recall indicates a low false negative rate, meaning that the model is effective at capturing positive instances.\n",
    "\n",
    "In summary, precision focuses on the accuracy of positive predictions, while recall focuses on the completeness of capturing positive instances. Both metrics are important and need to be balanced depending on the specific application or problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643b7029",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Questions 2.2\n",
    "\n",
    "1. Looking at the results on the test data, which is your model better at: catching true crops that exist or not labeling non-crops as crops? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a59ceb",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove explanation\n",
    "\"\"\"\n",
    "1. The recall score is higher, suggesting that the model is good at capturing the true cropland, but precision is lower, indicating it does classify non-crop land as cropland.\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092e6678",
   "metadata": {
    "execution": {}
   },
   "source": [
    "\n",
    "## Section 2.3:  Another Feature Importance Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b49386b",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Coding Exercise 2.3\n",
    "\n",
    "Create a series of new datasets, each of which contains only 11 out of the 12 independent variables. Train and evaluate the accuracy of models using these different reduced datasets. Comparing the performances of these different models is one way of trying to understand which input features are most important for correct classification. Try also training a series of models that only use one feature at a time. According to these results, are there any features that seem especially useful for performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0yPWd4NJxTpq",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def calculate_performance(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Calculates performance with missing one features and only one feature using logistic regression.\n",
    "\n",
    "    Args:\n",
    "        X_train (ndarray): The training data.\n",
    "        X_test (ndarray): The testing data.\n",
    "        y_train (ndarray): The training labels.\n",
    "        y_test (ndarray): The testing labels.\n",
    "\n",
    "    Returns:\n",
    "        missing_feature_performance (list): A list of performance scores for each missing feature.\n",
    "        only_feature_performance (list): A list of performance scores for each individual feature.\n",
    "    \"\"\"\n",
    "\n",
    "    missing_feature_performance = (\n",
    "        []\n",
    "    )  # create empty list to store performance scores for each missing feature\n",
    "    only_feature_performance = (\n",
    "        []\n",
    "    )  # create empty list to store performance scores for each individual feature\n",
    "\n",
    "    for feature in range(\n",
    "        X_train.shape[1]\n",
    "    ):  # iterate through each feature in the dataset\n",
    "        # remove the feature from both the training and test set\n",
    "        X_train_reduced = np.delete(\n",
    "            X_train, feature, 1\n",
    "        )  # remove feature from training set\n",
    "        X_test_reduced = np.delete(X_test, feature, 1)  # remove feature from test set\n",
    "        # train a logistic regression model on the reduced training set\n",
    "        reduced_trained_model = ...\n",
    "        missing_feature_performance.append(\n",
    "            ...\n",
    "        )  # calculate the score on the reduced test set and append to list\n",
    "\n",
    "        # select only the feature from both the training and test set\n",
    "        X_train_reduced = X_train[:, feature].values.reshape(\n",
    "            -1, 1\n",
    "        )  # select only feature from training set\n",
    "        X_train_reduced = X_train[:, feature].values.reshape(\n",
    "            -1, 1\n",
    "        )  # select only feature from training set\n",
    "        # train a logistic regression model on the reduced training set\n",
    "        reduced_trained_model = ...\n",
    "\n",
    "        only_feature_performance.append(...)\n",
    "        # calculate the score on the reduced test set and append to list\n",
    "\n",
    "    return missing_feature_performance, only_feature_performance\n",
    "\n",
    "\n",
    "missing_feature_performance, only_feature_performance = calculate_performance(\n",
    "    X_train, X_test, y_train, y_test\n",
    ")  # Call calculate_performance() function with the training and testing data and the corresponding labels\n",
    "# Uncomment next line\n",
    "# plot_feature_performance(missing_feature_performance, only_feature_performance, feature_names) # Plot the performance scores for each missing feature and each individual feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Rv-TvK3PwpzD",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "\n",
    "def calculate_performance(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Calculates performance with missing features and only one feature using logistic regression.\n",
    "\n",
    "    Args:\n",
    "        X_train (ndarray): The training data.\n",
    "        X_test (ndarray): The testing data.\n",
    "        y_train (ndarray): The training labels.\n",
    "        y_test (ndarray): The testing labels.\n",
    "\n",
    "    Returns:\n",
    "        missing_feature_performance (list): A list of performance scores for each missing feature.\n",
    "        only_feature_performance (list): A list of performance scores for each individual feature.\n",
    "    \"\"\"\n",
    "\n",
    "    missing_feature_performance = (\n",
    "        []\n",
    "    )  # create empty list to store performance scores for each missing feature\n",
    "    only_feature_performance = (\n",
    "        []\n",
    "    )  # create empty list to store performance scores for each individual feature\n",
    "\n",
    "    for feature in range(\n",
    "        X_train.shape[1]\n",
    "    ):  # iterate through each feature in the dataset\n",
    "        # remove the feature from both the training and test set\n",
    "        X_train_reduced = np.delete(\n",
    "            X_train, feature, 1\n",
    "        )  # remove feature from training set\n",
    "        X_test_reduced = np.delete(X_test, feature, 1)  # remove feature from test set\n",
    "        reduced_trained_model = LogisticRegression().fit(\n",
    "            X_train_reduced, y_train\n",
    "        )  # train a logistic regression model on the reduced training set\n",
    "        missing_feature_performance.append(\n",
    "            reduced_trained_model.score(X_test_reduced, y_test)\n",
    "        )  # calculate the score on the reduced test set and append to list\n",
    "\n",
    "        # select only the feature from both the training and test set\n",
    "        X_train_reduced = X_train[:, feature].values.reshape(\n",
    "            -1, 1\n",
    "        )  # select only feature from training set\n",
    "        X_test_reduced = X_test[:, feature].values.reshape(\n",
    "            -1, 1\n",
    "        )  # select only feature from test set\n",
    "        # train a logistic regression model on the reduced training set\n",
    "        reduced_trained_model = LogisticRegression().fit(\n",
    "            X_train_reduced, y_train\n",
    "        )\n",
    "        only_feature_performance.append(reduced_trained_model.score(X_test_reduced, y_test))\n",
    "        # calculate the score on the reduced test set and append to list\n",
    "\n",
    "    return missing_feature_performance, only_feature_performance\n",
    "\n",
    "\n",
    "missing_feature_performance, only_feature_performance = calculate_performance(\n",
    "    X_train, X_test, y_train, y_test\n",
    ")  # Call calculate_performance() function with the training and testing data and the corresponding labels\n",
    "# Uncomment next line\n",
    "# plot_feature_performance(missing_feature_performance, only_feature_performance, feature_names) # Plot the performance scores for each missing feature and each individual feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t-7mzn4sWM0p",
   "metadata": {
    "execution": {}
   },
   "source": [
    "<details>\n",
    "<summary> <font color='green'>Click here for interpretation of plot  (first try to understand by yourself)  </font></summary>\n",
    "The plot shows the performance of the logistic regression model when each feature is removed from the dataset (missing feature performance) and when each feature is used individually as the only predictor (only feature performance). The height of each bar represents the performance score for the corresponding model. Therefore, in the left plot, the lower the bar the more important the feature is (performance drops when that feature is not included). In the right plot, the higher the bar the more important the feature is (the model can perform well with that feature alone)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nnE6lSZiMd9K",
   "metadata": {
    "execution": {}
   },
   "source": [
    "\n",
    "\n",
    "## (Bonus) Section 2.4: Compare to Permutation Feature importance\n",
    "\n",
    "Use what you learned to also implement the permutation method of estimating feature importance on this model. Compare results to the other feature importance methods, and to your predictions about what features would be useful based on the histograms plotted above. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "orxTe50z8llC",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Coding Exercise 2.4\n",
    "For this exercise, you have to evaluate and plot feature importance with the permutation method using the `permutation_importance` function from [sklearn.inspection](https://scikit-learn.org/stable/modules/generated/sklearn.inspection.permutation_importance.html). (A reminder, you have used this method in Tutorial 2 Section 3.2)\n",
    "\n",
    "Here are the steps to follow:\n",
    "\n",
    "1. Calculate the permutation feature importance using trained_model, X_test, and y_test.\n",
    "2. Set the number of repeats to 10 and random state to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "krUkiudIMvMT",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Evaluate and plot feature importance with the permutation method\n",
    "\n",
    "# calculate the permutation feature importance using trained_model, X_test and y_test\n",
    "perm_feat_imp = ...\n",
    "\n",
    "# Uncomment next line\n",
    "# plot_permutation_feature_importance(perm_feat_imp, X_test, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CCmrovB4ghE8",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "# Evaluate and plot feature importance with the permutation method\n",
    "\n",
    "# calculate the permutation feature importance using trained_model, X_test and y_test\n",
    "perm_feat_imp = permutation_importance(\n",
    "    trained_model, X_test, y_test, n_repeats=10, random_state=0\n",
    ")\n",
    "\n",
    "# Uncomment next line\n",
    "# plot_permutation_feature_importance(perm_feat_imp, X_test, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZVYzbk0J_c9A",
   "metadata": {
    "execution": {}
   },
   "source": [
    "<details>\n",
    "<summary> <font color='green'>Click here for description of code  </font></summary>\n",
    "The code is performing feature importance analysis using the permutation importance method. This method is used to determine the importance of features in a machine learning model by shuffling the values of each feature and measuring the effect on the model's performance.\n",
    "\n",
    "The first step is to import the necessary libraries including `sklearn` for the permutation importance method. Then the permutation importance is computed using the `permutation_importance()` function, which takes the trained model, test data, and number of repeats as input.\n",
    "\n",
    "The results of the permutation importance are stored in the `perm_feat_imp` variable.\n",
    "\n",
    "Finally, a bar plot is generated using Matplotlib to visualize the feature importance values. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Q9wYRJRO82Nn",
   "metadata": {
    "execution": {}
   },
   "source": [
    "<details>\n",
    "<summary> <font color='green'>Click here for interpretation of plot  </font><font color=' red'>(first try to understand by yourself)  </font></summary>\n",
    "The plot generated shows the feature importance scores using the permutation importance method. Each bar represents the importance of a feature, with the height of the bar indicating the mean importance score across all permutations and the error bar indicating the standard deviation.\n",
    "\n",
    "Features with higher importance scores are considered more important in predicting the target variable than those with lower scores. You can use this information to identify the most important features in your dataset and potentially reduce the dimensionality of your model by removing the least important features.\n",
    "\n",
    "In this particular plot, the x-axis represents the feature index (or feature number), and the y-axis represents the feature importance score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8l4r-WlalZ0B",
   "metadata": {
    "execution": {}
   },
   "source": [
    "\n",
    "# Section 3: Artificial Neural Networks on Crops Dataset\n",
    "\n",
    "\n",
    "An artificial neural network (ANN) is a computational model inspired by the structure and function of the human brain. It consists of multiple interconnected layers of artificial neurons that learn to recognize patterns and make predictions from input data. \n",
    "\n",
    "The input layer receives the data, and the output layer produces the model's predictions. The hidden layers between the input and output layers are where the network's computations take place. Each node in a hidden layer performs a weighted sum of the inputs and applies an activation function to the result, producing an output that is sent to the next layer. The weights and biases of the nodes in the network are learned during training, where the model is fed data and adjusts its weights and biases to minimize the error between its predictions and the true values.\n",
    "\n",
    "ANNs have been successfully applied to a variety of climate-related problems, such as climate modeling, extreme weather event prediction, and crop yield forecasting. ANNs can also help to overcome some of the limitations of traditional statistical methods by allowing for non-linear relationships between variables and handling large, complex datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v8hCuPdpl8FV",
   "metadata": {
    "execution": {}
   },
   "source": [
    "\n",
    "## Section 3.1: Training an Artificial Neural Network (ANN) on Crops Data\n",
    "\n",
    "As discussed before, there are usually multiple possible ways to solve a data science problem. Here we will train an artificial neural network (ANN) to perform binary classification on our remote sensing crops data set. \n",
    "\n",
    "In the following section, we will be using the data loaded in the previous section. Specifically, we have the training data in the variables `X_train` and `y_train`, and the testing data in the variables `X_test` and `y_test`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NAO0bYT4u6FZ",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Small artificial neural networks can be trained in scitkit-learn just like other models. Use [MLPClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier) with default parameters to train a model on the crops training set and evaluate its accuracy on both the training and test sets. (MLP stands for multi-layer perceptron, another name for a simple artificial neural network). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uvzZ_prJl8FV",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# set a random seed to ensure reproducibility of results\n",
    "np.random.seed(144)\n",
    "\n",
    "# fit the MLPClassifier model on the training data by calling .fit() on MLPClassifier()\n",
    "trained_model = ...\n",
    "\n",
    "# print the training accuracy and test accuracy of the model\n",
    "print(\" Training Accuracy: \", ...)\n",
    "print(\" Test Accuracy:     \", ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f32a750",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "\n",
    "# set a random seed to ensure reproducibility of results\n",
    "np.random.seed(144)\n",
    "\n",
    "# fit the MLPClassifier model on the training data by calling .fit() on MLPClassifier()\n",
    "trained_model = MLPClassifier().fit(X_train, y_train)\n",
    "\n",
    "# print the training accuracy and test accuracy of the model\n",
    "print(\" Training Accuracy: \", trained_model.score(X_train, y_train))\n",
    "print(\" Test Accuracy:     \", trained_model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2bc147",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Questions 3.1\n",
    "\n",
    "1. Does the ANN perform better or worse than the logistic regression model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e3f7ac",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove explanation\n",
    "\"\"\"\n",
    "1. The ANN performs better on the training data, but the same on the test data compared to logistic regression.\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WRASeTezl8FW",
   "metadata": {
    "execution": {}
   },
   "source": [
    "\n",
    "## Section 3.2: Overfitting in ANNs\n",
    "Our ANN model has better training performance but not better test performance than the linear regression model. As we saw previously, high training data performance but poor test data performance could be a sign of overfitting. Overfitting happens when a model has too many free parameters, which it uses to 'memorize' the relationships between inputs and outputs in the training set. In this way, what the model learns is too specific to the training data and won't be helpful on new data. \n",
    "\n",
    "One way to cause overfitting is to use a large model. Use the hidden_layer_sizes parameter of MLPClassifier to vary the number of units in the hidden layer in your ANN. Plot the training and test performance as a function of the number of hidden units. Note how training performance and testing performance show different trends.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5PoDL-uT21DJ",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Coding Exercise 3.2\n",
    "\n",
    "Visualizing the Impact of Hidden Units on the Performance of a Multi-Layer Perceptron Classifier\n",
    "\n",
    "**Objective**: In this exercise, you are required to fill in the missing code in the `mlp_performance` function. The function is supposed to train multiple MLP classifiers with different numbers of hidden units and return the training and testing accuracy for each classifier.\n",
    "\n",
    "It may take a few minutes for this to run. While you wait, discuss what adding more units really means. What does each hidden unit do and how does having more give the model higher capacity? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tOyc0mrb_WOu",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def plot_mlp_performance(hidden_units, train_accuracy, test_accuracy):\n",
    "    \"\"\"\n",
    "    Plot the performance of the Multi-Layer Perceptron (MLP) model with varying hidden units.\n",
    "\n",
    "    Args:\n",
    "    hidden_units (range): A range of integers representing the different number of hidden units.\n",
    "    train_accuracy (list): A list of floats representing the training accuracy for each number of hidden units.\n",
    "    test_accuracy (list): A list of floats representing the testing accuracy for each number of hidden units.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    # create a figure and axis object with specified size\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    # plot the training accuracy as a function of number of hidden units\n",
    "    ax.plot(\n",
    "        hidden_units,\n",
    "        train_accuracy,\n",
    "        marker=\"o\",\n",
    "        label=\"Training Accuracy\",\n",
    "        color=\"navy\",\n",
    "        alpha=0.8,\n",
    "        linewidth=2,\n",
    "    )\n",
    "\n",
    "    # plot the testing accuracy as a function of number of hidden units\n",
    "    ax.plot(\n",
    "        hidden_units,\n",
    "        test_accuracy,\n",
    "        marker=\"o\",\n",
    "        label=\"Testing Accuracy\",\n",
    "        color=\"crimson\",\n",
    "        alpha=0.8,\n",
    "        linewidth=2,\n",
    "    )\n",
    "\n",
    "    # add a legend to the plot\n",
    "    ax.legend()\n",
    "\n",
    "    # add a title to the plot\n",
    "    ax.set_title(\n",
    "        \"Performance of MLP with Varying Hidden Units\", fontsize=16, fontweight=\"bold\"\n",
    "    )\n",
    "\n",
    "    # add x-label to the plot\n",
    "    ax.set_xlabel(\"Number of Hidden Units\", fontsize=14)\n",
    "\n",
    "    # add y-label to the plot\n",
    "    ax.set_ylabel(\"Accuracy\", fontsize=14)\n",
    "\n",
    "    # set tick labels for x and y axes\n",
    "    ax.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n",
    "\n",
    "    # add gridlines to the plot\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "    # remove top and right spines of the plot\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "\n",
    "    # adjust the layout of the plot\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NA3oHuvrA2UO",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def mlp_performance(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    This function trains multiple MLP classifiers with different numbers of hidden units and returns the training and testing\n",
    "    accuracy for each classifier.\n",
    "\n",
    "    Args:\n",
    "    X_train (ndarray): An array of shape (N_train, M) that contains the training data\n",
    "    y_train (ndarray): An array of shape (N_train,) that contains the labels for the training data\n",
    "    X_test (ndarray): An array of shape (N_test, M) that contains the testing data\n",
    "    y_test (ndarray): An array of shape (N_test,) that contains the labels for the testing data\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple of three arrays: hidden_units, train_accuracy, and test_accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    # define the range of hidden units to test\n",
    "    hidden_units = range(5, 300, 15)  # Test 5 to 300 hidden units in steps of 15\n",
    "\n",
    "    # create empty lists to store training and testing accuracy for different numbers of hidden units\n",
    "    train_accuracy = []\n",
    "    test_accuracy = []\n",
    "\n",
    "    # loop over different numbers of hidden units\n",
    "    for units in hidden_units:\n",
    "        # set a seed for random number generation to ensure consistent results\n",
    "        np.random.seed(144)\n",
    "\n",
    "        # create an MLP classifier with the current number of hidden units\n",
    "        classifier = MLPClassifier(hidden_layer_sizes=(...))\n",
    "\n",
    "        # train the classifier on the training data\n",
    "        classifier.fit(..., y_train)\n",
    "\n",
    "        # calculate the training and testing accuracy of the classifier\n",
    "        train_acc = classifier.score(..., ...)\n",
    "        test_acc = classifier.score(..., ...)\n",
    "\n",
    "        # add the training and testing accuracy to the corresponding lists\n",
    "        train_accuracy.append(...)\n",
    "        test_accuracy.append(...)\n",
    "\n",
    "    return hidden_units, train_accuracy, test_accuracy\n",
    "\n",
    "\n",
    "# call mlp_performance function\n",
    "# Uncomment next line\n",
    "# hidden_units, train_accuracy, test_accuracy = mlp_performance(X_train, y_train, X_test, y_test)\n",
    "# plot the training and testing accuracy as a function of the number of hidden units\n",
    "# Uncomment next line\n",
    "# plot_mlp_performance(hidden_units, train_accuracy, test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p5VJ14mEL3jc",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "def mlp_performance(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    This function trains multiple MLP classifiers with different numbers of hidden units and returns the training and testing\n",
    "    accuracy for each classifier.\n",
    "\n",
    "    Args:\n",
    "    X_train (ndarray): An array of shape (N_train, M) that contains the training data\n",
    "    y_train (ndarray): An array of shape (N_train,) that contains the labels for the training data\n",
    "    X_test (ndarray): An array of shape (N_test, M) that contains the testing data\n",
    "    y_test (ndarray): An array of shape (N_test,) that contains the labels for the testing data\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple of three arrays: hidden_units, train_accuracy, and test_accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    # define the range of hidden units to test\n",
    "    hidden_units = range(5, 300, 15)  # Test 5 to 300 hidden units in steps of 15\n",
    "\n",
    "    # create empty lists to store training and testing accuracy for different numbers of hidden units\n",
    "    train_accuracy = []\n",
    "    test_accuracy = []\n",
    "\n",
    "    # loop over different numbers of hidden units\n",
    "    for units in hidden_units:\n",
    "        # set a seed for random number generation to ensure consistent results\n",
    "        np.random.seed(144)\n",
    "\n",
    "        # create an MLP classifier with the current number of hidden units\n",
    "        classifier = MLPClassifier(hidden_layer_sizes=(units))\n",
    "\n",
    "        # train the classifier on the training data\n",
    "        classifier.fit(X_train, y_train)\n",
    "\n",
    "        # calculate the training and testing accuracy of the classifier\n",
    "        train_acc = classifier.score(X_train, y_train)\n",
    "        test_acc = classifier.score(X_test, y_test)\n",
    "\n",
    "        # add the training and testing accuracy to the corresponding lists\n",
    "        train_accuracy.append(train_acc)\n",
    "        test_accuracy.append(test_acc)\n",
    "\n",
    "    return hidden_units, train_accuracy, test_accuracy\n",
    "\n",
    "\n",
    "# call mlp_performance function\n",
    "# Uncomment next line\n",
    "# hidden_units, train_accuracy, test_accuracy = mlp_performance(X_train, y_train, X_test, y_test)\n",
    "# plot the training and testing accuracy as a function of the number of hidden units\n",
    "# Uncomment next line\n",
    "# plot_mlp_performance(hidden_units, train_accuracy, test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jtVA_2hSSScp",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Questions 3.2\n",
    "\n",
    "1. How do you interpret the plot above? What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36241e52",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove explanation\n",
    "\"\"\"\n",
    "1. The plot generated by the mlp_performance function shows the training and testing accuracy of a multi-layer perceptron classifier (MLP) as a function of the number of hidden units. The blue line shows the training accuracy, while the orange line shows the testing accuracy.  As the number of hidden units increases, the training accuracy generally increases as well. However, the testing accuracy may not necessarily increase and may start to plateau or even decrease at some point. This indicates the model is overfitting, where it is learning the noise in the training data instead of the underlying pattern.\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-DX-Nzzyl8Fa",
   "metadata": {
    "execution": {}
   },
   "source": [
    "\n",
    "\n",
    "## Section 3.3: Overfitting by Learning Too Much\n",
    "\n",
    "As the network is trained, it loops through all the training data repeatedly, updating its weights to make the network perform better on that data. Another way to induce overfitting is to let the network loop over the training data too many times. Repeat the above training experiment with the default number of hidden units but varying the number of training epochs using the max_iter parameter. \n",
    "\n",
    "While you wait for this to run, you can continue your discussion about hidden units. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FFug21_4Npxy",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Coding Exercise 3.3\n",
    "\n",
    "1. Training MLPClassifier with varying number of epochs and observe performance. Here more epochs means we let the network loop over the data more times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tNm6Vt8kJIf5",
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def plot_training_performance(train_perfs, test_perfs):\n",
    "    \"\"\"\n",
    "    Plots the training and test performance against the number of training epochs.\n",
    "\n",
    "    Args:\n",
    "    train_perfs: list of floats representing the training performance for each epoch.\n",
    "    test_perfs: list of floats representing the test performance for each epoch.\n",
    "    \"\"\"\n",
    "    # create a figure and axis object\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    # plot the training and test performance against the number of training epochs\n",
    "    ax.plot(\n",
    "        np.arange(5, 500, 20),\n",
    "        train_perfs,\n",
    "        label=\"Training Accuracy\",\n",
    "        linewidth=2,\n",
    "        marker=\"o\",\n",
    "    )\n",
    "    ax.plot(\n",
    "        np.arange(5, 500, 20),\n",
    "        test_perfs,\n",
    "        label=\"Test Accuracy\",\n",
    "        linewidth=2,\n",
    "        marker=\"s\",\n",
    "    )\n",
    "    ax.set_title(\n",
    "        \"Training and Test Performance vs. Number of Training Epochs\",\n",
    "        fontsize=16,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "    ax.set_xlabel(\"Number of Training Epochs\", fontsize=14)\n",
    "    ax.set_ylabel(\"Accuracy\", fontsize=14)\n",
    "    ax.legend(loc=\"lower right\", fontsize=12)\n",
    "    ax.grid(True)\n",
    "\n",
    "    # add text annotation for maximum test accuracy\n",
    "    max_test_acc = max(test_perfs)\n",
    "    max_test_epoch = (np.argmax(test_perfs) * 20) + 5\n",
    "    ax.annotate(\n",
    "        f\"Maximum test accuracy of {max_test_acc:.2f} at {max_test_epoch} epochs\",\n",
    "        xy=(max_test_epoch, max_test_acc),\n",
    "        xytext=(max_test_epoch + 50, max_test_acc - 0.05),\n",
    "        fontsize=12,\n",
    "        fontweight=\"bold\",\n",
    "        color=\"green\",\n",
    "        arrowprops=dict(facecolor=\"green\", shrink=0.05),\n",
    "    )\n",
    "\n",
    "    # show the plot\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mbB1zZitLxTd",
   "metadata": {
    "execution": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_epochs(\n",
    "    X_train, y_train, X_test, y_test, start_epoch=5, end_epoch=500, step=20\n",
    "):\n",
    "    \"\"\"\n",
    "    Train an MLPClassifier for a range of epochs and return the training and test accuracy for each epoch.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train: array-like of shape (n_samples, n_features)\n",
    "        Training input samples.\n",
    "    y_train: array-like of shape (n_samples,)\n",
    "        Target values for the training set.\n",
    "    X_test: array-like of shape (n_samples, n_features)\n",
    "        Test input samples.\n",
    "    y_test: array-like of shape (n_samples,)\n",
    "        Target values for the test set.\n",
    "    start_epoch: int, optional\n",
    "        The first epoch to train for. Default is 5.\n",
    "    end_epoch: int, optional\n",
    "        The last epoch to train for. Default is 500.\n",
    "    step: int, optional\n",
    "        The step size between epochs. Default is 20.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        Two lists, the first containing the training accuracy for each epoch, and the second containing the test\n",
    "        accuracy for each epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize empty lists to store training and test performance\n",
    "    train_accuracy = []\n",
    "    test_accuracy = []\n",
    "\n",
    "    # Loop through a range of epochs and train the MLPClassifier model for each epoch\n",
    "    for m in range(start_epoch, end_epoch, step):\n",
    "        # Set random seed for reproducibility\n",
    "        np.random.seed(144)\n",
    "\n",
    "        # Fit the MLPClassifier model to the training data for the current epoch\n",
    "        trained_model = MLPClassifier(max_iter=m).fit(..., ...)\n",
    "\n",
    "        # Calculate and store the training and test accuracy for the current epoch\n",
    "        train_accuracy.append(trained_model.score(..., ...))\n",
    "        test_accuracy.append(trained_model.score(..., ...))\n",
    "\n",
    "    # Return the lists of training and test performance for each epoch\n",
    "    return train_accuracy, test_accuracy\n",
    "\n",
    "\n",
    "# call the train_epochs function and store the returned values in variables train_perfs and test_perfs\n",
    "# Uncomment next line\n",
    "# train_perfs, test_perfs = train_epochs(X_train, y_train, X_test, y_test)\n",
    "\n",
    "# calling plotting function to plot the performance vs epochs plot\n",
    "# Uncomment next line\n",
    "# plot_training_performance(train_perfs, test_perfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3LpVn2UBJx0X",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "\n",
    "\n",
    "def train_epochs(\n",
    "    X_train, y_train, X_test, y_test, start_epoch=5, end_epoch=500, step=20\n",
    "):\n",
    "    \"\"\"\n",
    "    Train an MLPClassifier for a range of epochs and return the training and test accuracy for each epoch.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train: array-like of shape (n_samples, n_features)\n",
    "        Training input samples.\n",
    "    y_train: array-like of shape (n_samples,)\n",
    "        Target values for the training set.\n",
    "    X_test: array-like of shape (n_samples, n_features)\n",
    "        Test input samples.\n",
    "    y_test: array-like of shape (n_samples,)\n",
    "        Target values for the test set.\n",
    "    start_epoch: int, optional\n",
    "        The first epoch to train for. Default is 5.\n",
    "    end_epoch: int, optional\n",
    "        The last epoch to train for. Default is 500.\n",
    "    step: int, optional\n",
    "        The step size between epochs. Default is 20.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        Two lists, the first containing the training accuracy for each epoch, and the second containing the test\n",
    "        accuracy for each epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize empty lists to store training and test performance\n",
    "    train_accuracy = []\n",
    "    test_accuracy = []\n",
    "\n",
    "    # Loop through a range of epochs and train the MLPClassifier model for each epoch\n",
    "    for m in range(start_epoch, end_epoch, step):\n",
    "        # Set random seed for reproducibility\n",
    "        np.random.seed(144)\n",
    "\n",
    "        # Fit the MLPClassifier model to the training data for the current epoch\n",
    "        trained_model = MLPClassifier(max_iter=m).fit(X_train, y_train)\n",
    "\n",
    "        # Calculate and store the training and test accuracy for the current epoch\n",
    "        train_accuracy.append(trained_model.score(X_train, y_train))\n",
    "        test_accuracy.append(trained_model.score(X_test, y_test))\n",
    "\n",
    "    # Return the lists of training and test performance for each epoch\n",
    "    return train_accuracy, test_accuracy\n",
    "\n",
    "\n",
    "# call the train_epochs function and store the returned values in variables train_perfs and test_perfs\n",
    "# Uncomment next line\n",
    "train_perfs, test_perfs = train_epochs(X_train, y_train, X_test, y_test)\n",
    "\n",
    "# calling plotting function to plot the performance vs epochs plot\n",
    "# Uncomment next line\n",
    "plot_training_performance(train_perfs, test_perfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "G2oZTsHiUIgP",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The plot generated shows the training and test accuracy of a neural network as the number of training epochs increases. The training accuracy generally increases with the number of epochs, while the test accuracy initially increases but eventually levels off or even decreases due to overfitting. The maximum test accuracy achieved and the epoch at which it occurs can also be seen on the plot. As this and the previous plot show, choosing the right parameters when buiding and training artificial neural networks is very important to prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QStFtvNLMtCX",
   "metadata": {
    "execution": {}
   },
   "source": [
    "\n",
    "## Section 3.4: Reflection on Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "K2PhZ9T4l8Fd",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Question 3.4\n",
    "\n",
    "Now that we saw what can cause overfitting, reflect on how to prevent it. In addition to controlling the number of hidden units and amount of training, are there any other ways to prevent overfitting? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d98b4c-0a14-447c-9ffd-7b0f1d810051",
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "#  to_remove explanation\n",
    "\"\"\"\n",
    "Solutions to handle overfitting include reducing model complexity, increasing dataset size, using regularization, or cross-validation. Ensemble models such as random forests also do inherently help control overfitting by averaging many different models\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DiR7mcnycPiB",
   "metadata": {
    "execution": {}
   },
   "source": [
    "\n",
    "# Summary\n",
    "\n",
    "The tutorial covered how to analyze and visualize the crop dataset, train a logistic regression model, and evaluate its performance using various metrics. Feature importance was also discussed, and two methods were explored. In addition, the tutorial covered training an artificial neural network on the crops dataset and preventing overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6e57f9",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Resources\n",
    "\n",
    "The data for the tutorial can be accessed [here](https://github.com/nasaharvest/cropharvest) and is described in [this paper](https://arxiv.org/pdf/2006.16866.pdf). The jupyter notebook used to download this data can be accessed [here](https://github.com/ClimateMatchAcademy/course-content/blob/main/tutorials/W2D5_ClimateResponse-AdaptationImpact/Get_Crop_Harvest_Data.ipynb)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "W2D5_Tutorial3",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
