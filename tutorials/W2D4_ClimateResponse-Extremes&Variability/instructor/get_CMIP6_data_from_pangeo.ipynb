{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7ff59c1",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ClimateMatchAcademy/course-content/blob/main/tutorials/W2D4_ClimateResponse-Extremes&Variability/get_CMIP6_data_from_pangeo.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e69bb7f",
   "metadata": {},
   "source": [
    "# Load CMIP6 Data from Pangeo\n",
    "\n",
    "The code in this notebook was used to create the input data for Tutorial 6 using the CMIP6 data linked to the Pangeo cloud on Google Cloud. It is provided here to give a better idea of how data can be obtained through [Pangeo](https://pangeo.io/). Its machinery of working with CMIP (and other) data is very powerful and immensely useful. It was decided to pre-compute the data and load this precomputed data in Tutorial 7 as it is not necessary (and too time-consuming) that every participant runs this operation, unnecessarily using computing resources. \n",
    "\n",
    "In this way you can access large amounts of climate model output that has been stored in the cloud. This is very useful to get easy access to such information. Feel free to modify the code to access different data and address your own questions.\n",
    "\n",
    "Before accessing data this way, please familiarize yourself with the Pangeo documentation. \n",
    "This code works by setting up a large, compliated query, resulting in the `data` object. The actual computation is triggered by running the last cell, after uncommenting the lines\n",
    "```\n",
    "#with ProgressBar():\n",
    "#     data.load()\n",
    "```\n",
    "\n",
    "Please DO NOT simply uncomment this line and just run it straightaway, the code takes quite long time to run and uses significant computing resources. Think first what is it that you need.\n",
    "\n",
    "However, all the preceeding steps are quite cheap, and can give you a good idea of what data exists on Pangeo, and what can be done with it.\n",
    "\n",
    "Note that you will have to be on a cloud-based system that can access Google Cloud - you will likely not be able to run this on your laptop. \n",
    "\n",
    "Get excited, go explore, have fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33c25f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import intake\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from xmip.preprocessing import combined_preprocessing\n",
    "from xarrayutils.plotting import shaded_line_plot\n",
    "\n",
    "from xmip.utils import google_cmip_col\n",
    "\n",
    "col = google_cmip_col()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affcbf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = col.search(\n",
    "    source_id=[\"MPI-ESM1-2-HR\", \"MIROC6\"],\n",
    "    variable_id=[\"pr\", \"tas\"],\n",
    "    member_id=\"r1i1p1f1\",  #\n",
    "    table_id=\"day\",\n",
    "    grid_label=\"gn\",\n",
    "    experiment_id=[\"historical\", \"ssp126\", \"ssp245\", \"ssp585\"],\n",
    "    require_all_on=[\"experiment_id\", \"variable_id\"],\n",
    ")\n",
    "kwargs = dict(\n",
    "    preprocess=combined_preprocessing, xarray_open_kwargs=dict(use_cftime=True)\n",
    ")\n",
    "ds_dict = cat.to_dataset_dict(zarr_kwargs={\"consolidated\": True})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ecce435d",
   "metadata": {},
   "source": [
    "Define one or more locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d7f1be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_Hamburg = dict(lon=10, lat=53.5)\n",
    "sel_Madrid = dict(lon=360 - 42, lat=40.5)\n",
    "sel_Delhi = dict(lon=77, lat=28.5)\n",
    "sel_Kinshasa = dict(lon=15, lat=-4)\n",
    "sel_Phoenix = dict(lon=360 - 112, lat=33.5)\n",
    "sel_Sydney = dict(lon=151, lat=-33.85)\n",
    "\n",
    "sels = dict(\n",
    "    Hamburg=sel_Hamburg,\n",
    "    Madrid=sel_Madrid,\n",
    "    Delhi=sel_Delhi,\n",
    "    Kinshasa=sel_Kinshasa,\n",
    "    Phoenix=sel_Phoenix,\n",
    "    Sydney=sel_Sydney,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "296d94e7",
   "metadata": {},
   "source": [
    "Assemble your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5236ff74",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "model_dict = {}\n",
    "for k in ds_dict.keys():\n",
    "    string = k.split(\".\")\n",
    "    model = string[2]\n",
    "    models.append(model)\n",
    "    scenario = string[3]\n",
    "    print(string, model, scenario)\n",
    "    model_dict[\"%s.%s\" % (model, scenario)] = k\n",
    "\n",
    "models = [\"MPI-ESM1-2-HR\", \"MIROC6\"]\n",
    "scenarios = [\"ssp126\", \"ssp245\", \"ssp585\"]\n",
    "outs = []\n",
    "for model in models:\n",
    "    outsm = []\n",
    "    for city in sels.keys():\n",
    "        outis = []\n",
    "        for scenario in scenarios:\n",
    "            timeseries = xr.concat(\n",
    "                [\n",
    "                    ds_dict[model_dict[\"%s.%s\" % (model, \"historical\")]]\n",
    "                    .sel(sels[city], method=\"nearest\")\n",
    "                    .sel(time=slice(\"2014\")),\n",
    "                    ds_dict[model_dict[\"%s.%s\" % (model, scenario)]]\n",
    "                    .sel(time=slice(\"2100\"))\n",
    "                    .sel(sels[city], method=\"nearest\")\n",
    "                    # ds_dict['%s.%s' % (model, 'historical')].sel(sels[city],method='nearest').sel(time=slice('2014')),\n",
    "                    # ds_dict['%s.%s' % (model, scenario)].sel(sels[city],method='nearest')\n",
    "                ],\n",
    "                \"time\",\n",
    "            )\n",
    "            timeseries = timeseries.assign_coords(\n",
    "                city=city, scenario=scenario, model=model\n",
    "            ).squeeze()\n",
    "            outis.append(timeseries)\n",
    "            # outis.append(ds_dict[key].sel(sels[city],method='nearest').assign_coords(city=city,scenario=key).squeeze())\n",
    "        outsm.append(xr.concat(outis, \"scenario\"))\n",
    "    outs.append(xr.concat(outsm, \"city\"))\n",
    "data = xr.concat(outs, \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5d532f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The data has %.3f MB\" % data.nbytes / 1e6)\n",
    "print(\"The next line triggers the computation!\")\n",
    "# with ProgressBar():\n",
    "#     data.load()\n",
    "\n",
    "# Fix time axis\n",
    "data = data.assign_coords(time=pd.to_datetime(data.time))\n",
    "# Convert precip data to mm/day\n",
    "data[\"pr\"] = data[\"pr\"] * 86400\n",
    "data[\"pr\"].attrs = data[\"pr\"].attrs\n",
    "data[\"pr\"].attrs[\"units\"] = \"mm/day\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climatematch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
