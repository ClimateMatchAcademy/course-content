# @title #**Project background** 
#This will be a short video introducing the content creator(s) and motivating the research direction of the template.
#The Tech team will add code to format and display the video

# Imports

#Import only the libraries/objects that are necessary for more than one dataset. 
#Dataset-specific imports should be in the respective notebook section.

#If any external library has to be installed, !pip install library --quiet
#follow this order: numpy>matplotlib. 
#import widgets in hidden Figure settings cell


import numpy as np
import matplotlib.pyplot as plt
import xarray as xr

from google.colab import drive
drive.mount('/content/drive')

# Functions

#Only functions that apply to more than one data source (MODIS, CMIP, ERA5, ...) should be here.
#Functions that apply to more than one datafile from a dataset (MODIS: land cover type and MODIS: NPP) 
#should be in the respective section of the notebook.

# Wrap it into a simple function
def seasonal_mean_by_year(ds, last_month, rolling_months=3 ):
    """
    This function calculates the seasonal mean by resampling the input dataset
    to monthly means, applying a rolling mean with the specified season length.
    The resulting dataset is then filtered to select the last month of the 
    corresponding season, which represents the seasonal mean.
    
    Parameters
    ----------
    ds : xarray.Dataset
        The input dataset containing a time dimension.
    last_month : int
        The last month of the seasonal group. For example, if the season is DJF
        (December-January-February), then last_month=2 (for February).
    rolling_months : int, optional
        The number of months for the rolling mean window, by default 3.
        
    Returns
    -------
    xarray.Dataset
        The seasonal mean dataset.
    """
    #Resampling data to monthly means
    ds_ = ds.resample(time = '1M').mean()

    #Rolling mean. 
    ds_ = ds_.rolling(time = rolling_months).mean()

    #Select month to get the average over selected season
    return ds_.sel(time=ds_.time.dt.month == last_month)

# @title Helper functions


##### Coments for main function
# The above function serves mainly for Q2 and Q3. 
# If you want to calculate the seasonal mean for a custom season (e.g. ONDJFM), 
# you can implement the function as follows:
# data_ONDJFM = seasonal_mean_by_year(data,3,rolling_months=6)

#### helper function: 
def standardize_data(da, dim):
    """
    Performs standardization on a DataArray along a specified dimension.
    
    Parameters:
        - da (xarray.DataArray): Input DataArray to be standardized.
        - dim (str): Dimension along which to perform standardization.
        
    Returns:
        - xarray.DataArray: Standardized DataArray.
    """
    # Calculate mean and standard deviation along the specified dimension
    mean = da.mean(dim=dim)
    std = da.std(dim=dim)
    
    # Perform standardization
    standardized_da = (da - mean) / std
    
    return standardized_da

def calculate_sdii_index(data):
    """
    This function calculates the Simple Daily Intensity Index (SDII), which
    represents the average amount of precipitation on wet days (days with
    precipitation greater than or equal to 1mm) for each year in the input data.
    The input data should be a Dataset with time coordinates, and the function
    returns a Dataset with the SDII index values for each year in the data.
    ----------
    - data (xarray.Dataset): Input dataset containing daily precipitation data.
    - period (str, optional): Period for which to calculate the SDII index. 
      
    Returns:
    -------
        - xarray.Dataset: Dataset containing the SDII index for the given period.
    """
    # Calculate daily precipitation amount on wet days (PR >= 1mm)
    wet_days = data.where(data >= 1)
    
    # Group by year and calculate the sum precipitation on wet days
    sum_wet_days_grouped = wet_days.groupby('time.year').sum(dim='time')

    # Count number of wet days for each time step
    w = wet_days.groupby('time.year').count(dim='time')

    # Divide by the number of wet days to get SDII index
    sdii = sum_wet_days_grouped/w
    
    return sdii

def calculate_cdd_index(data):
    """
    This function takes a daily precipitation dataset as input and calculates
    the Consecutive Dry Days (CDD) index, which represents the longest sequence
    of consecutive days with precipitation less than 1mm. The input data should
    be a DataArray with time coordinates, and the function returns a DataArray
    with the CDD values for each unique year in the input data.   
    Parameters:
    ----------
      - data (xarray.DataArray): The input daily precipitation data should be 
      a dataset (eg. for chirps_data the SataArray would be chirps_data.precip)
    Returns:
    -------
      - cdd (xarray.DataArray): The calculated CDD index
     
    """
    # Create a boolean array for dry days (PR < 1mm)
    dry_days = data < 1 
     # Initialize CDD array
    cdd = np.zeros(len(data.groupby("time.year"))) 
    # Get unique years as a list
    unique_years = list(data.groupby("time.year").groups.keys())  
    #Iterate for each day
    for i, year in enumerate(unique_years):
      consecutive_trues = []
      current_count = 0
      for day in dry_days.sel(time=data["time.year"] == year).values:
          if day:
              current_count += 1
          else:
              if current_count > 0:
                  consecutive_trues.append(current_count)
                  current_count = 0
      if current_count > 0:
          consecutive_trues.append(current_count)
      #print(consecutive_trues)
      #CDD is the largest number of consecutive days 
      cdd[i] = np.max(consecutive_trues)
    #Transform to dataset
    cdd_da = xr.DataArray(cdd, coords={"year": unique_years}, dims="year")
    return cdd_da

# Code to retrieve and load the data

#### this works after mounting my drive directory and should be changed (Laura)
#files='drive/MyDrive/project_notebook/chirps-v2.0.*'

###Accessing the shared Drive folder (Raphael)
# open_mfdataset doesn't work with wildcards for remote access.
# Workaround retrieving a list of paths with !ls
files_pattern = 'chirps-v2.0.*'
files_dir= 'drive/Shareddrives/Academy/Courses/Climate/Climatematch/06-Projects/01-Resources/Data\ exploration\ notebooks/Regional\ precipitation\ variability\ \&\ extreme\ events\ \(Rapha\ \&\ Laura\)/CHIRPS/'
files_SList = !ls {files_dir}/{files_pattern} #(returns a SList object)
files_list = list(files_SList) # transform to list
files = [f.strip('\'') for f in files_list] # removes extra single quotes

#### Load Data
chirps_data = xr.open_mfdataset(files,combine='by_coords')

## It would be also good to point to the tutorial on Extreme events, although 
## this is covered by the end of the course

# Code to print the shape, array names, etc of the dataset
chirps_data

files_pattern = 'NDVI*'
files_dir= 'drive/Shareddrives/Academy/Courses/Climate/Climatematch/06-Projects/01-Resources/Data\ exploration\ notebooks/Regional\ precipitation\ variability\ \&\ extreme\ events\ \(Rapha\ \&\ Laura\)/MODIS/'
files_SList = !ls {files_dir}/{files_pattern} #(returns a SList object)
files_list = list(files_SList) # transform to list
files = [f.strip('\'') for f in files_list] # removes extra single quotes

#### Load Data
modis_data = xr.open_mfdataset(files,combine='by_coords')


modis_data

import pandas as pd

# Code to retrieve and load the data
files= 'drive/Shareddrives/Academy/Courses/Climate/Climatematch/06-Projects/01-Resources/Data exploration notebooks/Regional precipitation variability & extreme events (Rapha & Laura)/data_cereal_land_meta.csv'
ds_cereal_land = pd.read_csv(files)
ds_cereal_land.head() 

##Example
ds_cereal_land[(ds_cereal_land['Country Name']=='Brazil')].reset_index(drop=True).iloc[0].transpose()


